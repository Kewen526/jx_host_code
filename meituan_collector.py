#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¾å›¢ç‚¹è¯„æ•°æ®é‡‡é›†ç»Ÿä¸€å…¥å£ - å•æ–‡ä»¶ç‰ˆæœ¬

ç›´æ¥åœ¨PyCharmä¸­è¿è¡Œï¼Œä¿®æ”¹ä¸‹æ–¹çš„é…ç½®å‚æ•°å³å¯
åŒ…å«: 6ä¸ªæ•°æ®é‡‡é›†ä»»åŠ¡ + store_statsé—¨åº—ç»Ÿè®¡ä»»åŠ¡(ä½¿ç”¨Playwrightæµè§ˆå™¨)
"""

import json
import time
import random
import requests
import pandas as pd
import math
import os
import sys
import subprocess
import signal
from typing import Dict, Any, Optional, List
from pathlib import Path
from datetime import datetime, timedelta
from io import BytesIO

# Playwrightå¯¼å…¥ (ç”¨äºstore_statsä»»åŠ¡)
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸ æœªå®‰è£…playwrightï¼Œstore_statsä»»åŠ¡å°†ä¸å¯ç”¨")
    print("   å®‰è£…æ–¹æ³•: pip install playwright && playwright install chromium")


# ============================================================================
# â˜…â˜…â˜… åœ¨è¿™é‡Œä¿®æ”¹é…ç½®å‚æ•° â˜…â˜…â˜…
# ============================================================================
ACCOUNT_NAME = "13718175572a"       # è´¦æˆ·åç§° (å¿…å¡«)
START_DATE = "2025-12-01"           # å¼€å§‹æ—¥æœŸ (å¿…å¡«, æ ¼å¼: YYYY-MM-DD)
END_DATE = "2025-12-15"             # ç»“æŸæ—¥æœŸ (å¿…å¡«, æ ¼å¼: YYYY-MM-DD)
TARGET_DATE = ""                    # store_statsç›®æ ‡æ—¥æœŸ (ç•™ç©º=ä½¿ç”¨END_DATE)
TASK = "all"                        # ä»»åŠ¡åç§° (å¿…å¡«)
                                    # å¯é€‰å€¼:
                                    #   "all" - é¡µé¢é©±åŠ¨æ¨¡å¼ï¼Œå…ˆè·³è½¬é¡µé¢å†æ‰§è¡Œä»»åŠ¡
                                    #           é¡ºåº: æŠ¥è¡¨é¡µé¢ â†’ å®¢æµåˆ†æé¡µé¢ â†’ è¯„ä»·é¡µé¢
                                    #   "store_stats" - é—¨åº—ç»Ÿè®¡(å¼ºåˆ¶ä¸‹çº¿ã€å®¢æµã€æ’åç­‰)
                                    #   "kewen_daily_report" - å®¢æ–‡æ—¥æŠ¥
                                    #   "promotion_daily_report" - æ¨å¹¿æ—¥æŠ¥
                                    #   "review_detail_dianping" - ç‚¹è¯„è¯„ä»·æ˜ç»†
                                    #   "review_detail_meituan" - ç¾å›¢è¯„ä»·æ˜ç»†
                                    #   "review_summary_dianping" - ç‚¹è¯„è¯„ä»·æ±‡æ€»
                                    #   "review_summary_meituan" - ç¾å›¢è¯„ä»·æ±‡æ€»

# store_stats æµè§ˆå™¨é…ç½®
HEADLESS = True                     # æµè§ˆå™¨æ¨¡å¼: True=åå°è¿è¡Œ, False=æ˜¾ç¤ºçª—å£

# ============================================================================
# â˜…â˜…â˜… å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼é…ç½® â˜…â˜…â˜…
# ============================================================================
DEV_MODE = True                     # å¼€å‘æ¨¡å¼: True=24å°æ—¶è¿è¡Œ, False=ä»…åœ¨å·¥ä½œæ—¶é—´è¿è¡Œ
WORK_START_HOUR = 8                 # å·¥ä½œå¼€å§‹æ—¶é—´ (ä»…DEV_MODE=Falseæ—¶ç”Ÿæ•ˆ)
WORK_END_HOUR = 23                  # å·¥ä½œç»“æŸæ—¶é—´ (ä»…DEV_MODE=Falseæ—¶ç”Ÿæ•ˆ)
NO_TASK_WAIT_SECONDS = 300          # æ— ä»»åŠ¡æ—¶ç­‰å¾…ç§’æ•° (5åˆ†é’Ÿ)

# ============================================================================
# â˜…â˜…â˜… è·¯å¾„é…ç½® (æœåŠ¡å™¨éƒ¨ç½²æ—¶ä½¿ç”¨ç»å¯¹è·¯å¾„) â˜…â˜…â˜…
# ============================================================================
DATA_DIR = "/home/meituan/data"                     # æ•°æ®æ ¹ç›®å½•
STATE_DIR = "/home/meituan/data/state"              # CookieçŠ¶æ€æ–‡ä»¶ç›®å½•
DOWNLOAD_DIR = "/home/meituan/data/downloads"       # ä¸‹è½½æ–‡ä»¶ç›®å½•

# ============================================================================
# APIé…ç½® (ä¸€èˆ¬ä¸éœ€è¦ä¿®æ”¹)
# ============================================================================
COOKIE_API_URL = "http://8.146.210.145:3000/api/get_namecookies"
PLATFORM_ACCOUNTS_API_URL = "http://8.146.210.145:3000/api/get_platform_accounts"
LOG_API_URL = "http://8.146.210.145:3000/api/log"
AUTH_STATUS_API_URL = "http://8.146.210.145:3000/api/post/platform_accounts"  # ç™»å½•çŠ¶æ€ä¸ŠæŠ¥API
TASK_STATUS_BATCH_API_URL = "http://8.146.210.145:3000/api/account_task/update_batch"  # ä»»åŠ¡çŠ¶æ€æ‰¹é‡ä¸ŠæŠ¥API
TASK_STATUS_SINGLE_API_URL = "http://8.146.210.145:3000/api/account_task/update_single"  # ä»»åŠ¡çŠ¶æ€å•ç‹¬ä¸ŠæŠ¥API
TASK_SCHEDULE_API_URL = "http://8.146.210.145:3000/api/post_task_schedule"  # ä»»åŠ¡è°ƒåº¦ç”ŸæˆAPI
GET_TASK_API_URL = "http://8.146.210.145:3000/api/get_task"  # è·å–ä»»åŠ¡API
TASK_CALLBACK_API_URL = "http://8.146.210.145:3000/api/task/callback"  # ä»»åŠ¡å®Œæˆå›è°ƒAPI
RESCHEDULE_FAILED_API_URL = "http://8.146.210.145:3000/api/task/reschedule-failed"  # å¤±è´¥ä»»åŠ¡é‡æ–°è°ƒåº¦API
GET_PLATFORM_ACCOUNT_API_URL = "http://8.146.210.145:3000/api/get_platform_account"  # è·å–å¹³å°è´¦æˆ·ä¿¡æ¯API
SAVE_DIR = DOWNLOAD_DIR  # ä½¿ç”¨ç»å¯¹è·¯å¾„

# å„ä»»åŠ¡çš„ä¸Šä¼ API
UPLOAD_APIS = {
    "store_stats": "http://8.146.210.145:3000/api/store_stats",
    "kewen_daily_report": "http://8.146.210.145:3000/api/kewen_daily_report",
    "promotion_daily_report": "http://8.146.210.145:3000/api/promotion_daily_report",
    "review_detail_dianping": "http://8.146.210.145:3000/api/review_detail_dianping",
    "review_detail_meituan": "http://8.146.210.145:3000/api/review_detail_meituan",
    "review_summary_dianping": "http://8.146.210.145:3000/api/review_summary_dianping",
    "review_summary_meituan": "http://8.146.210.145:3000/api/review_summary_meituan",
}

# ============================================================================
# é¡µé¢é©±åŠ¨ä»»åŠ¡é…ç½® - å…ˆè·³è½¬é¡µé¢å†æ‰§è¡Œå¯¹åº”ä»»åŠ¡
# ============================================================================
PAGE_URLS = {
    # æŠ¥è¡¨é¡µé¢ - æ‰§è¡Œ kewen_daily_report, promotion_daily_report
    "report": "https://e.dianping.com/app/merchant-platform/0fb1bec0bade47d?iUrl=Ly9oNS5kaWFucGluZy5jb20vdmctcGMtYWR2aWNlL3JlcG9ydC1jZW50ZXIvaW5kZXguaHRtbA",
    # å®¢æµåˆ†æé¡µé¢ - æ‰§è¡Œ store_stats
    "flow_analysis": "https://e.dianping.com/app/merchant-platform/468ccfd01240492?iUrl=Ly9oNS5kaWFucGluZy5jb20vdmctcGMtYWR2aWNlL2FkdmljZS1mbG93LWFuYWx5c2lzL2luZGV4Lmh0bWw",
    # è¯„ä»·é¡µé¢ - æ‰§è¡Œ review_detail_dianping, review_detail_meituan, review_summary_dianping, review_summary_meituan
    "review": "https://e.dianping.com/app/merchant-platform/7dfe97aa7164460?iUrl=Ly9lLmRpYW5waW5nLmNvbS92Zy1wbGF0Zm9ybS1yZXZpZXdtYW5hZ2Uvc2hvcC1jb21tZW50LWRwL2luZGV4Lmh0bWw",
}

# é¡µé¢ä¸ä»»åŠ¡çš„æ˜ å°„å…³ç³»
PAGE_TASKS = {
    "report": ["kewen_daily_report", "promotion_daily_report"],
    "flow_analysis": ["store_stats"],
    "review": ["review_detail_dianping", "review_detail_meituan", "review_summary_dianping", "review_summary_meituan"],
}

# é¡µé¢æ‰§è¡Œé¡ºåºï¼ˆå®¢æµåˆ†æå…ˆæ‰§è¡Œä»¥æ›´æ–°ç­¾åï¼Œè¯„ä»·é¡µé¢æ”¾æœ€åï¼‰
PAGE_ORDER = ["flow_analysis", "report", "review"]

# ============================================================================
# å…±äº«ç­¾åå­˜å‚¨ (store_statsæ‰§è¡Œåæ›´æ–°ï¼Œä¾›å…¶ä»–ä»»åŠ¡ä½¿ç”¨)
# ============================================================================
SHARED_SIGNATURE = {
    'mtgsig': None,          # ç­¾åå­—ç¬¦ä¸²
    'cookies': None,         # æ›´æ–°åçš„cookies
    'updated_at': None,      # æ›´æ–°æ—¶é—´
    'shop_list': None,       # é—¨åº—åˆ—è¡¨
}


# ============================================================================
# å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼: ä¿¡å·å¤„ç†å’Œæ—¶é—´çª—å£æ§åˆ¶
# ============================================================================
# å…¨å±€è¿è¡Œæ ‡å¿— (ç”¨äºä¼˜é›…é€€å‡º)
_daemon_running = True


def _signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å‡½æ•°ï¼Œç”¨äºä¼˜é›…é€€å‡º"""
    global _daemon_running
    _daemon_running = False
    sig_name = signal.Signals(signum).name if hasattr(signal, 'Signals') else str(signum)
    print(f"\n{'=' * 60}")
    print(f"âš ï¸ æ”¶åˆ°é€€å‡ºä¿¡å· ({sig_name})ï¼Œç­‰å¾…å½“å‰ä»»åŠ¡å®Œæˆåé€€å‡º...")
    print(f"{'=' * 60}")


def _setup_signal_handlers():
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
    signal.signal(signal.SIGINT, _signal_handler)   # Ctrl+C
    signal.signal(signal.SIGTERM, _signal_handler)  # killå‘½ä»¤
    print("âœ… å·²è®¾ç½®ä¿¡å·å¤„ç†å™¨ (æ”¯æŒCtrl+Cä¼˜é›…é€€å‡º)")


def is_in_work_window() -> bool:
    """æ£€æŸ¥å½“å‰æ˜¯å¦åœ¨å·¥ä½œæ—¶é—´çª—å£å†…

    DEV_MODE=True æ—¶å§‹ç»ˆè¿”å›True (24å°æ—¶è¿è¡Œ)
    DEV_MODE=False æ—¶æ£€æŸ¥æ˜¯å¦åœ¨ WORK_START_HOUR è‡³ WORK_END_HOUR ä¹‹é—´
    """
    if DEV_MODE:
        return True
    current_hour = datetime.now().hour
    return WORK_START_HOUR <= current_hour < WORK_END_HOUR


def seconds_until_work_start() -> int:
    """è®¡ç®—è·ç¦»ä¸‹ä¸€ä¸ªå·¥ä½œæ—¶é—´å¼€å§‹çš„ç§’æ•°"""
    now = datetime.now()
    if now.hour >= WORK_END_HOUR:
        # ä»Šå¤©å·²è¿‡å·¥ä½œç»“æŸæ—¶é—´ï¼Œç­‰åˆ°æ˜å¤©å¼€å§‹æ—¶é—´
        next_start = now.replace(hour=WORK_START_HOUR, minute=0, second=0, microsecond=0) + timedelta(days=1)
    elif now.hour < WORK_START_HOUR:
        # è¿˜æ²¡åˆ°ä»Šå¤©çš„å¼€å§‹æ—¶é—´
        next_start = now.replace(hour=WORK_START_HOUR, minute=0, second=0, microsecond=0)
    else:
        # å·²åœ¨å·¥ä½œæ—¶é—´å†…
        return 0
    return int((next_start - now).total_seconds())


def ensure_directories():
    """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
    directories = [DATA_DIR, STATE_DIR, DOWNLOAD_DIR]
    for dir_path in directories:
        try:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
            print(f"âœ… ç›®å½•å·²å°±ç»ª: {dir_path}")
        except Exception as e:
            print(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥ {dir_path}: {e}")
            raise


def interruptible_sleep(seconds: int, check_interval: int = 10) -> bool:
    """å¯ä¸­æ–­çš„ç¡çœ å‡½æ•°

    Args:
        seconds: æ€»ç¡çœ ç§’æ•°
        check_interval: æ£€æŸ¥é—´éš”ç§’æ•°

    Returns:
        bool: True=æ­£å¸¸å®Œæˆ, False=è¢«ä¸­æ–­
    """
    global _daemon_running
    elapsed = 0
    while elapsed < seconds and _daemon_running:
        sleep_time = min(check_interval, seconds - elapsed)
        time.sleep(sleep_time)
        elapsed += sleep_time
    return _daemon_running


# ============================================================================
# æ—¥å¿—ä¸ŠæŠ¥åŠŸèƒ½
# ============================================================================
def log_task_result(
    account_id: str,
    shop_id: int,
    table_name: str,
    data_date_start: str,
    data_date_end: str,
    upload_status: int,
    record_count: int = 0,
    error_message: str = "æ— "
) -> bool:
    """ä¸ŠæŠ¥ä»»åŠ¡æ‰§è¡Œæ—¥å¿—åˆ°API"""
    headers = {'Content-Type': 'application/json'}
    json_param = {
        "account_id": account_id,
        "shop_id": shop_id,
        "table_name": table_name,
        "data_date_start": data_date_start,
        "data_date_end": data_date_end,
        "upload_status": upload_status,
        "record_count": record_count,
        "error_message": error_message
    }
    proxies = {'http': None, 'https': None}

    print(f"\n{'â”€' * 50}")
    print(f"ğŸ“ æ—¥å¿—ä¸ŠæŠ¥è¯·æ±‚:")
    print(f"   URL: {LOG_API_URL}")
    print(f"   è¯·æ±‚å‚æ•°: {json.dumps(json_param, ensure_ascii=False, indent=6)}")

    try:
        response = requests.post(LOG_API_URL, headers=headers, data=json.dumps(json_param), proxies=proxies, timeout=30)
        print(f"   HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"   å“åº”å†…å®¹: {response.text[:500] if response.text else '(ç©º)'}")
        if response.status_code == 200:
            print(f"   âœ… æ—¥å¿—ä¸ŠæŠ¥æˆåŠŸ")
            return True
        else:
            print(f"   âŒ æ—¥å¿—ä¸ŠæŠ¥å¤±è´¥")
            return False
    except Exception as e:
        print(f"   âŒ æ—¥å¿—ä¸ŠæŠ¥å¼‚å¸¸: {e}")
        return False


def log_success(account_id: str, shop_id: int, table_name: str, data_date_start: str, data_date_end: str, record_count: int) -> bool:
    return log_task_result(account_id, shop_id, table_name, data_date_start, data_date_end, 2, record_count, "æ— ")


def log_failure(account_id: str, shop_id: int, table_name: str, data_date_start: str, data_date_end: str, error_message: str) -> bool:
    return log_task_result(account_id, shop_id, table_name, data_date_start, data_date_end, 1, 0, error_message)


# ============================================================================
# é€šç”¨å·¥å…·å‡½æ•°
# ============================================================================
def disable_proxy():
    """ç¦ç”¨ç³»ç»Ÿä»£ç†"""
    for key in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy', 'ALL_PROXY', 'all_proxy']:
        os.environ.pop(key, None)
    os.environ['NO_PROXY'] = '*'
    os.environ['no_proxy'] = '*'
    print("âœ… å·²ç¦ç”¨ç³»ç»Ÿä»£ç†")


def get_session() -> requests.Session:
    """è·å–ç¦ç”¨ä»£ç†çš„session"""
    session = requests.Session()
    session.trust_env = False
    session.proxies = {'http': None, 'https': None}
    return session


def report_auth_invalid(account_name: str) -> bool:
    """ä¸ŠæŠ¥è´¦æˆ·ç™»å½•å¤±æ•ˆçŠ¶æ€åˆ°API

    å½“çŠ¶æ€æ–‡ä»¶ç™»å½•å¤±è´¥ä¸”APIè¿”å›çš„cookieç™»å½•ä¹Ÿå¤±è´¥åè°ƒç”¨æ­¤å‡½æ•°
    """
    print(f"\n{'â”€' * 50}")
    print(f"ğŸ”” ä¸ŠæŠ¥è´¦æˆ·ç™»å½•å¤±æ•ˆçŠ¶æ€...")

    headers = {'Content-Type': 'application/json'}
    json_param = {
        "account": account_name,
        "auth_status": "invalid"
    }
    proxies = {'http': None, 'https': None}

    try:
        response = requests.post(
            AUTH_STATUS_API_URL,
            headers=headers,
            data=json.dumps(json_param),
            proxies=proxies,
            timeout=30
        )
        print(f"   URL: {AUTH_STATUS_API_URL}")
        print(f"   è¯·æ±‚å‚æ•°: {json.dumps(json_param, ensure_ascii=False)}")
        print(f"   HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"   å“åº”å†…å®¹: {response.text[:500] if response.text else '(ç©º)'}")

        if response.status_code == 200:
            print(f"   âœ… è´¦æˆ·å¤±æ•ˆçŠ¶æ€ä¸ŠæŠ¥æˆåŠŸ")
            return True
        else:
            print(f"   âŒ è´¦æˆ·å¤±æ•ˆçŠ¶æ€ä¸ŠæŠ¥å¤±è´¥")
            return False
    except Exception as e:
        print(f"   âŒ è´¦æˆ·å¤±æ•ˆçŠ¶æ€ä¸ŠæŠ¥å¼‚å¸¸: {e}")
        return False


def upload_task_status_batch(account_id: str, start_date: str, end_date: str, results: List[Dict[str, Any]]) -> bool:
    """æ‰¹é‡ä¸ŠæŠ¥æ‰€æœ‰ä»»åŠ¡çŠ¶æ€åˆ°API

    Args:
        account_id: è´¦æˆ·ID
        start_date: æ•°æ®å¼€å§‹æ—¥æœŸ
        end_date: æ•°æ®ç»“æŸæ—¥æœŸ
        results: ä»»åŠ¡æ‰§è¡Œç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« task_name, success, record_count, error_message

    Returns:
        bool: ä¸ŠæŠ¥æ˜¯å¦æˆåŠŸ
    """
    print(f"\n{'â”€' * 50}")
    print(f"ğŸ“¤ æ‰¹é‡ä¸ŠæŠ¥ä»»åŠ¡çŠ¶æ€...")

    # å…¨éƒ¨7ä¸ªä»»åŠ¡çš„åç§°åˆ—è¡¨
    ALL_TASK_NAMES = [
        "store_stats",
        "kewen_daily_report",
        "promotion_daily_report",
        "review_detail_dianping",
        "review_detail_meituan",
        "review_summary_dianping",
        "review_summary_meituan",
    ]

    # æ„å»ºAPIè¯·æ±‚å‚æ•°
    json_param = {
        "account_id": account_id,
        "data_start_date": start_date,
        "data_end_date": end_date,
    }

    # å…ˆç”¨é»˜è®¤å€¼åˆå§‹åŒ–æ‰€æœ‰7ä¸ªä»»åŠ¡çš„çŠ¶æ€ (0=æœªæ‰§è¡Œ)
    for task_name in ALL_TASK_NAMES:
        json_param[f"{task_name}_status"] = 0  # æœªæ‰§è¡Œ
        json_param[f"{task_name}_records"] = 0
        json_param[f"{task_name}_error"] = None

    # ç”¨å®é™…æ‰§è¡Œç»“æœè¦†ç›–
    for result in results:
        task_name = result.get('task_name')
        if task_name not in ALL_TASK_NAMES:
            print(f"   âš ï¸ æœªçŸ¥ä»»åŠ¡åç§°: {task_name}ï¼Œè·³è¿‡")
            continue

        success = result.get('success', False)
        record_count = result.get('record_count', 0)
        error_message = result.get('error_message', 'æ— ')

        # çŠ¶æ€ç : success=True -> 2, success=False -> 3
        status = 2 if success else 3
        # é”™è¯¯ä¿¡æ¯: success=True -> None, success=False -> å®é™…é”™è¯¯ä¿¡æ¯
        error = None if success else error_message

        json_param[f"{task_name}_status"] = status
        json_param[f"{task_name}_records"] = record_count
        json_param[f"{task_name}_error"] = error

    headers = {'Content-Type': 'application/json'}
    proxies = {'http': None, 'https': None}

    print(f"   URL: {TASK_STATUS_BATCH_API_URL}")
    print(f"   è¯·æ±‚å‚æ•°: {json.dumps(json_param, ensure_ascii=False, indent=6)}")

    try:
        response = requests.post(
            TASK_STATUS_BATCH_API_URL,
            headers=headers,
            data=json.dumps(json_param),
            proxies=proxies,
            timeout=30
        )
        print(f"   HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"   å“åº”å†…å®¹: {response.text[:500] if response.text else '(ç©º)'}")

        if response.status_code == 200:
            print(f"   âœ… æ‰¹é‡ä»»åŠ¡çŠ¶æ€ä¸ŠæŠ¥æˆåŠŸ")
            return True
        else:
            print(f"   âŒ æ‰¹é‡ä»»åŠ¡çŠ¶æ€ä¸ŠæŠ¥å¤±è´¥")
            return False
    except Exception as e:
        print(f"   âŒ æ‰¹é‡ä»»åŠ¡çŠ¶æ€ä¸ŠæŠ¥å¼‚å¸¸: {e}")
        return False


def upload_task_status_single(account_id: str, start_date: str, end_date: str, result: Dict[str, Any]) -> bool:
    """å•ç‹¬ä¸ŠæŠ¥å•ä¸ªä»»åŠ¡çŠ¶æ€åˆ°API

    Args:
        account_id: è´¦æˆ·ID
        start_date: æ•°æ®å¼€å§‹æ—¥æœŸ
        end_date: æ•°æ®ç»“æŸæ—¥æœŸ
        result: å•ä¸ªä»»åŠ¡æ‰§è¡Œç»“æœï¼ŒåŒ…å« task_name, success, record_count, error_message

    Returns:
        bool: ä¸ŠæŠ¥æ˜¯å¦æˆåŠŸ
    """
    print(f"\n{'â”€' * 50}")
    print(f"ğŸ“¤ å•ç‹¬ä¸ŠæŠ¥ä»»åŠ¡çŠ¶æ€...")

    task_name = result.get('task_name')
    success = result.get('success', False)
    record_count = result.get('record_count', 0)
    error_message = result.get('error_message', 'æ— ')

    # çŠ¶æ€ç : success=True -> 2, success=False -> 3
    status = 2 if success else 3
    # é”™è¯¯ä¿¡æ¯: success=True -> None, success=False -> å®é™…é”™è¯¯ä¿¡æ¯
    error = None if success else error_message

    json_param = {
        "account_id": account_id,
        "data_start_date": start_date,
        "data_end_date": end_date,
        "task_name": task_name,
        "status": status,
        "record_count": record_count,
        "error_message": error
    }

    headers = {'Content-Type': 'application/json'}
    proxies = {'http': None, 'https': None}

    print(f"   URL: {TASK_STATUS_SINGLE_API_URL}")
    print(f"   è¯·æ±‚å‚æ•°: {json.dumps(json_param, ensure_ascii=False, indent=6)}")

    try:
        response = requests.post(
            TASK_STATUS_SINGLE_API_URL,
            headers=headers,
            data=json.dumps(json_param),
            proxies=proxies,
            timeout=30
        )
        print(f"   HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"   å“åº”å†…å®¹: {response.text[:500] if response.text else '(ç©º)'}")

        if response.status_code == 200:
            print(f"   âœ… å•ä¸ªä»»åŠ¡çŠ¶æ€ä¸ŠæŠ¥æˆåŠŸ")
            return True
        else:
            print(f"   âŒ å•ä¸ªä»»åŠ¡çŠ¶æ€ä¸ŠæŠ¥å¤±è´¥")
            return False
    except Exception as e:
        print(f"   âŒ å•ä¸ªä»»åŠ¡çŠ¶æ€ä¸ŠæŠ¥å¼‚å¸¸: {e}")
        return False


def random_delay(min_seconds: float = 2, max_seconds: float = 5):
    """éšæœºç­‰å¾…æŒ‡å®šèŒƒå›´çš„æ—¶é—´ï¼ˆåçˆ¬è™«æªæ–½ï¼‰

    Args:
        min_seconds: æœ€å°ç­‰å¾…ç§’æ•°ï¼Œé»˜è®¤2ç§’
        max_seconds: æœ€å¤§ç­‰å¾…ç§’æ•°ï¼Œé»˜è®¤5ç§’
    """
    delay = random.uniform(min_seconds, max_seconds)
    print(f"â³ åçˆ¬è™«ç­‰å¾… {delay:.1f} ç§’...")
    time.sleep(delay)


def load_cookies_from_api(account_name: str) -> Dict[str, Any]:
    """ä»APIåŠ è½½cookieså’Œç›¸å…³ä¿¡æ¯"""
    print(f"ğŸ” æ­£åœ¨ä»APIè·å–è´¦æˆ· [{account_name}] çš„cookie...")

    session = get_session()
    response = session.post(
        COOKIE_API_URL,
        headers={'Content-Type': 'application/json'},
        json={"name": account_name},
        timeout=30,
        proxies={'http': None, 'https': None}
    )
    response.raise_for_status()
    result = response.json()

    if not result.get('success'):
        raise Exception(f"APIè¿”å›å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")

    record = result.get('data', {})
    if not record:
        raise Exception(f"æœªæ‰¾åˆ°è´¦æˆ· [{account_name}] çš„cookieæ•°æ®")

    # è§£æcookies
    cookies_json = record.get('cookies_json')
    if isinstance(cookies_json, str):
        cookies = json.loads(cookies_json)
    else:
        cookies = cookies_json or {}

    # è§£æmtgsig
    mtgsig_data = record.get('mtgsig')
    if isinstance(mtgsig_data, str):
        mtgsig = mtgsig_data
    elif isinstance(mtgsig_data, dict):
        mtgsig = json.dumps(mtgsig_data)
    else:
        mtgsig = None

    # è§£æshop_info
    shop_info = record.get('shop_info', {})

    # è·å–templates_id
    templates_id = record.get('templates_id')

    print(f"âœ… æˆåŠŸåŠ è½½ {len(cookies)} ä¸ªcookies")

    return {
        'cookies': cookies,
        'mtgsig': mtgsig,
        'shop_info': shop_info,
        'templates_id': templates_id
    }


def get_shop_ids(shop_info) -> List[int]:
    """ä»shop_infoæå–é—¨åº—IDåˆ—è¡¨"""
    shop_ids = []
    if shop_info:
        if isinstance(shop_info, list):
            for shop in shop_info:
                if isinstance(shop, dict) and shop.get('shopId'):
                    shop_ids.append(int(shop.get('shopId')))
        elif isinstance(shop_info, dict) and shop_info.get('shopId'):
            shop_ids.append(int(shop_info.get('shopId')))
    return shop_ids if shop_ids else [0]


def get_platform_account(account: str) -> Dict[str, Any]:
    """è·å–å¹³å°è´¦æˆ·ä¿¡æ¯

    è°ƒç”¨ /api/get_platform_account è·å–è´¦æˆ·çš„å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
    - cookie: ç™»å½•å‡­è¯
    - mtgsig: ç­¾åä¿¡æ¯
    - templates_id: æŠ¥è¡¨æ¨¡æ¿ID
    - stores_json: é—¨åº—ä¿¡æ¯
    - auth_status: ç™»å½•çŠ¶æ€
    ç­‰

    Args:
        account: è´¦æˆ·åç§°ï¼ˆæ‰‹æœºå·ï¼‰

    Returns:
        dict: åŒ…å«è´¦æˆ·å®Œæ•´ä¿¡æ¯çš„å­—å…¸
            - success: æ˜¯å¦æˆåŠŸ
            - data: è´¦æˆ·æ•°æ®ï¼ˆæˆåŠŸæ—¶ï¼‰
            - error_message: é”™è¯¯ä¿¡æ¯ï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    print(f"\n{'â”€' * 50}")
    print(f"ğŸ” è·å–å¹³å°è´¦æˆ·ä¿¡æ¯: {account}")

    headers = {'Content-Type': 'application/json'}
    json_param = {"account": account}
    proxies = {'http': None, 'https': None}

    print(f"   URL: {GET_PLATFORM_ACCOUNT_API_URL}")
    print(f"   è¯·æ±‚å‚æ•°: {json.dumps(json_param, ensure_ascii=False)}")

    try:
        response = requests.post(
            GET_PLATFORM_ACCOUNT_API_URL,
            headers=headers,
            data=json.dumps(json_param),
            proxies=proxies,
            timeout=30
        )
        print(f"   HTTPçŠ¶æ€ç : {response.status_code}")

        if response.status_code == 200:
            result = response.json()
            if result.get('success'):
                data = result.get('data', {})
                templates_id = data.get('templates_id')
                auth_status = data.get('auth_status')
                stores_json = data.get('stores_json', [])

                print(f"   âœ… è·å–æˆåŠŸ")
                print(f"   templates_id: {templates_id}")
                print(f"   auth_status: {auth_status}")
                print(f"   é—¨åº—æ•°é‡: {len(stores_json) if stores_json else 0}")

                return {
                    'success': True,
                    'data': data,
                    'templates_id': templates_id,
                    'auth_status': auth_status,
                    'cookie': data.get('cookie'),
                    'mtgsig': data.get('mtgsig'),
                    'stores_json': stores_json
                }
            else:
                error_msg = result.get('message', 'è·å–è´¦æˆ·ä¿¡æ¯å¤±è´¥')
                print(f"   âŒ APIè¿”å›å¤±è´¥: {error_msg}")
                return {
                    'success': False,
                    'error_message': error_msg
                }
        else:
            error_msg = f"HTTPçŠ¶æ€ç : {response.status_code}"
            print(f"   âŒ è¯·æ±‚å¤±è´¥: {error_msg}")
            return {
                'success': False,
                'error_message': error_msg
            }
    except Exception as e:
        error_msg = f"è¯·æ±‚å¼‚å¸¸: {str(e)}"
        print(f"   âŒ {error_msg}")
        return {
            'success': False,
            'error_message': error_msg
        }


def generate_mtgsig(cookies: dict, mtgsig_from_api: str = None) -> str:
    """ç”Ÿæˆmtgsigç­¾åå‚æ•°

    ä¼˜å…ˆçº§: APIç­¾å > æœ¬åœ°ç”Ÿæˆï¼ˆæ¯æ¬¡ç”Ÿæˆæ–°æ—¶é—´æˆ³ï¼‰
    æ³¨æ„: ä¸å†ä½¿ç”¨å…±äº«ç­¾åï¼Œé¿å…ç­¾åè¿‡æœŸå¯¼è‡´ä»»åŠ¡å¤±è´¥
    """
    # 1. ä¼˜å…ˆä½¿ç”¨APIè¿”å›çš„ç­¾å
    if mtgsig_from_api:
        return mtgsig_from_api

    # 2. æœ¬åœ°ç”Ÿæˆæ–°ç­¾åï¼ˆæ¯æ¬¡ç”Ÿæˆæ–°æ—¶é—´æˆ³ï¼Œç¡®ä¿ç­¾åæœ‰æ•ˆï¼‰
    timestamp = int(time.time() * 1000)
    webdfpid = cookies.get('WEBDFPID', '')
    a3 = webdfpid.split('-')[0] if webdfpid and '-' in webdfpid else '5y24v3837yu856y40w99918z268u6v77801vv1w288197958zzvzwy74'

    mtgsig = {
        "a1": "1.2",
        "a2": timestamp,
        "a3": a3,
        "a5": "jBpEMWibZqnOfn+vAsi8yo/kZpK57yUmniEBsbeugiBk2/5nSVi4jUHwsaXt01Ll43X26NE4uABqljWc7M9e8mkBxcu=",
        "a6": "hs1.6kqTyxwalpmvA3xfWt6C4GOVXV8jTW1AytrgLRPiQXPPO3n3UQFIKWTiDGaeXmDJtn4MQEi7f+BMdUtXeeSaMXW9hYSgOd2UuD/+Lac4sqD5ssj0nZesRyvVbOWEeBmBx",
        "a8": "e64733017f50d5892bacd63100c4099c",
        "a9": "4.1.1,7,205",
        "a10": "31",
        "x0": 4,
        "d1": "c9332725bc86a957c5b3185975b58e79"
    }
    return json.dumps(mtgsig)


# ============================================================================
# kewen_daily_report ä»»åŠ¡
# ============================================================================
KEWEN_COLUMN_MAPPING = {
    0: ("report_date", "string"), 1: ("province", "string"), 2: ("city", "string"),
    3: ("shop_id", "number"), 4: ("shop_name", "string"), 5: ("dianping_star", "number"),
    6: ("meituan_star", "number"), 7: ("operation_score", "number"), 8: ("operation_level", "string"),
    9: ("promotion_cost", "number"), 10: ("merchant_cost", "number"), 11: ("platform_service_fee", "number"),
    12: ("commission_gtv", "number"), 13: ("exposure_users", "number"), 14: ("exposure_count", "number"),
    15: ("visit_users", "number"), 16: ("visit_count", "number"), 17: ("exposure_visit_rate", "string"),
    18: ("order_users", "number"), 19: ("lead_users", "number"), 20: ("intent_users", "number"),
    21: ("intent_rate", "string"), 22: ("new_collect_users", "number"), 23: ("total_collect_users", "number"),
    24: ("avg_stay_seconds", "number"), 25: ("promotion_exposure_count", "number"), 26: ("promotion_click_count", "number"),
    27: ("verify_sale_amount", "number"), 28: ("verify_after_discount", "number"), 29: ("verify_coupon_count", "number"),
    30: ("verify_order_count", "number"), 31: ("verify_person_count", "number"), 32: ("verify_new_customer", "number"),
    33: ("order_coupon_count", "number"), 34: ("order_sale_amount", "number"), 35: ("consult_users", "number"),
    36: ("consult_lead_count", "number"), 37: ("consult_lead_rate", "string"), 38: ("avg_response_seconds", "number"),
    39: ("reply_rate_30s", "string"), 40: ("reply_rate_5min", "string"), 41: ("refund_amount", "number"),
    42: ("refund_order_count", "number"), 43: ("refund_users", "number"), 44: ("complaint_count", "number"),
    45: ("compensation_order_count", "number"), 46: ("new_review_count", "number"), 47: ("new_good_review_count", "number"),
    48: ("new_medium_review_count", "number"), 49: ("new_bad_review_count", "number"), 50: ("bad_review_reply_rate", "string"),
    51: ("total_review_count", "number"), 52: ("total_bad_review_count", "number"),
    # ============ æ–°å¢å­—æ®µ: é—¨åº—ä¼˜æƒ ç  (BB-BHåˆ—) ============
    53: ("coupon_code_type", "string"),  # ç ç±»å‹
    54: ("coupon_pay_order_count", "number"),  # æ”¯ä»˜è®¢å•æ•°(ä¸ª)
    55: ("coupon_pay_amount", "number"),  # æ”¯ä»˜é‡‘é¢(å…ƒ)
    56: ("coupon_verify_amount", "number"),  # æ ¸é”€é‡‘é¢(å…ƒ)
    57: ("coupon_scan_users", "number"),  # æ‰«ç äººæ•°(äºº)
    58: ("coupon_scan_collect_count", "number"),  # æ‰«ç æ”¶è—æ•°(ä¸ª)
    59: ("coupon_scan_review_count", "number"),  # æ‰«ç è¯„ä»·æ•°(ä¸ª)
}

KEWEN_STRING_DEFAULTS = {
    "operation_level": "æš‚æ— ", "exposure_visit_rate": "0%", "intent_rate": "0%",
    "consult_lead_rate": "0%", "reply_rate_30s": "0%", "reply_rate_5min": "0%", "bad_review_reply_rate": "0%",
    "coupon_code_type": "",  # é—¨åº—ä¼˜æƒ ç ç±»å‹é»˜è®¤å€¼
}


def kewen_convert_value(value, data_type, field_name):
    """è½¬æ¢å€¼ä¸ºæŒ‡å®šç±»å‹"""
    if value is None or (isinstance(value, float) and math.isnan(value)) or (isinstance(value, str) and value.strip() == ''):
        if data_type == "number":
            return 0
        elif data_type == "string":
            return KEWEN_STRING_DEFAULTS.get(field_name, "")
        return None

    if data_type == "string":
        if hasattr(value, 'strftime'):
            return value.strftime("%Y-%m-%d")
        return str(value).strip()
    elif data_type == "number":
        try:
            if isinstance(value, str):
                value = value.replace(',', '')
            return float(value)
        except:
            return 0
    return value


def kewen_parse_excel_row(row):
    """è§£æExcelè¡Œæ•°æ®"""
    data = {}
    for col_idx, (field_name, data_type) in KEWEN_COLUMN_MAPPING.items():
        if col_idx < len(row):
            value = row.iloc[col_idx]
            converted = kewen_convert_value(value, data_type, field_name)
            if converted is not None:
                data[field_name] = converted
    return data


def kewen_is_empty_row(data):
    """æ£€æŸ¥æ˜¯å¦ä¸ºç©ºè¡Œ"""
    return not data.get('report_date') or data.get('shop_id', 0) == 0 or not data.get('shop_name')


def kewen_is_valid_coupon_type(data):
    """
    æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ä¼˜æƒ ç ç±»å‹ï¼ˆåªä¿ç•™"å…¨éƒ¨ç "ï¼‰

    Args:
        data: è§£æåçš„è¡Œæ•°æ®

    Returns:
        True: æ˜¯"å…¨éƒ¨ç "ï¼Œåº”è¯¥ä¿ç•™
        False: æ˜¯å…¶ä»–ç±»å‹ï¼ˆé—¨åº—ç /å•†å“ç /èŒäººç /å“ç‰Œç ï¼‰ï¼Œåº”è¯¥è·³è¿‡
    """
    coupon_code_type = data.get('coupon_code_type', '')
    return coupon_code_type == 'å…¨éƒ¨ç '


def run_kewen_daily_report(account_name: str, start_date: str, end_date: str) -> Dict[str, Any]:
    """æ‰§è¡Œkewen_daily_reportä»»åŠ¡"""
    table_name = "kewen_daily_report"
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š {table_name}")
    print(f"{'=' * 60}")

    result = {"task_name": table_name, "success": False, "record_count": 0, "error_message": "æ— "}

    try:
        disable_proxy()
        Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)

        # åŠ è½½cookies
        api_data = load_cookies_from_api(account_name)
        cookies = api_data['cookies']
        mtgsig = api_data['mtgsig']
        shop_info = api_data['shop_info']
        templates_id = api_data['templates_id']

        if not templates_id:
            raise Exception("æœªè·å–åˆ°æŠ¥è¡¨æ¨¡æ¿ID")

        shop_ids = get_shop_ids(shop_info)

        headers = {
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Referer': 'https://e.dianping.com/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        session = get_session()

        # æŠ¥è¡¨ä¸‹è½½é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3æ¬¡ï¼‰
        MAX_RETRY_ATTEMPTS = 3
        RETRY_DELAY_SECONDS = 10
        file_record = None
        last_error_message = ""

        for retry_attempt in range(1, MAX_RETRY_ATTEMPTS + 1):
            print(f"\nğŸ” æ­£åœ¨è¯·æ±‚ç”ŸæˆæŠ¥è¡¨... (ç¬¬ {retry_attempt}/{MAX_RETRY_ATTEMPTS} æ¬¡å°è¯•)")
            url = "https://e.dianping.com/gateway/adviser/report/template/download"
            params = {
                'source': '1', 'device': 'pc', 'id': templates_id,
                'date': f"{start_date},{end_date}",
                'yodaReady': 'h5', 'csecplatform': '4', 'csecversion': '4.1.1',
                'mtgsig': generate_mtgsig(cookies, mtgsig)
            }
            response = session.get(url, params=params, headers=headers, cookies=cookies, timeout=30)
            resp_json = response.json()
            print(f"ğŸ“Š è¯·æ±‚å“åº”: {resp_json}")

            # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
            result_type = resp_json.get('data', {}).get('resultType')
            if result_type == 3:
                # æœåŠ¡å¼‚å¸¸ï¼Œéœ€è¦é‡è¯•
                last_error_message = f"æœåŠ¡å¼‚å¸¸ (resultType={result_type})"
                print(f"âš ï¸ ç¬¬ {retry_attempt} æ¬¡å°è¯•å¤±è´¥: {last_error_message}")
                if retry_attempt < MAX_RETRY_ATTEMPTS:
                    print(f"   ç­‰å¾… {RETRY_DELAY_SECONDS} ç§’åé‡è¯•...")
                    time.sleep(RETRY_DELAY_SECONDS)
                    continue
                else:
                    raise Exception(f"æŠ¥è¡¨ä¸‹è½½é‡è¯• {MAX_RETRY_ATTEMPTS} æ¬¡å‡å¤±è´¥: {last_error_message}")

            random_delay()  # åçˆ¬è™«ç­‰å¾…

            # ç­‰å¾…æŠ¥è¡¨ç”Ÿæˆ
            print(f"\nâ³ ç­‰å¾…æŠ¥è¡¨ç”Ÿæˆ...")
            date_keyword = f"{start_date.replace('-', '')}-{end_date.replace('-', '')}"

            for _ in range(60):
                time.sleep(2)
                list_url = "https://e.dianping.com/gateway/merchant/downloadcenter/list"
                list_params = {'pageNo': 1, 'pageSize': 20, 'yodaReady': 'h5', 'csecplatform': '4', 'csecversion': '4.1.1', 'mtgsig': generate_mtgsig(cookies, mtgsig)}
                list_resp = session.get(list_url, params=list_params, headers=headers, cookies=cookies, timeout=30)
                list_data = list_resp.json()

                if list_data.get('code') == 200:
                    for record in list_data.get('data', {}).get('records', []):
                        if record.get('recordStatus') == 300 and record.get('downloadable') == "1" and record.get('fileUrl'):
                            if date_keyword in record.get('fileName', ''):
                                file_record = record
                                print(f"   âœ… æ–‡ä»¶å·²å°±ç»ª: {record.get('fileName')}")
                                break
                if file_record:
                    break

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æˆåŠŸç”Ÿæˆ
            if file_record:
                print(f"âœ… ç¬¬ {retry_attempt} æ¬¡å°è¯•æˆåŠŸï¼Œæ–‡ä»¶å·²å°±ç»ª")
                break  # æˆåŠŸè·å–æ–‡ä»¶ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
            else:
                # æ–‡ä»¶æœªç”Ÿæˆï¼Œéœ€è¦é‡è¯•
                last_error_message = "æŠ¥è¡¨ç”Ÿæˆè¶…æ—¶ï¼Œæ–‡ä»¶æœªå°±ç»ª"
                print(f"âš ï¸ ç¬¬ {retry_attempt} æ¬¡å°è¯•å¤±è´¥: {last_error_message}")
                if retry_attempt < MAX_RETRY_ATTEMPTS:
                    print(f"   ç­‰å¾… {RETRY_DELAY_SECONDS} ç§’åé‡è¯•...")
                    time.sleep(RETRY_DELAY_SECONDS)
                    continue
                else:
                    raise Exception(f"æŠ¥è¡¨ä¸‹è½½é‡è¯• {MAX_RETRY_ATTEMPTS} æ¬¡å‡å¤±è´¥: {last_error_message}")

        random_delay()  # åçˆ¬è™«ç­‰å¾…

        # ä¸‹è½½æ–‡ä»¶
        file_url = file_record['fileUrl']
        file_name = file_record.get('fileName', f'report_{templates_id}.xlsx')
        save_path = str(Path(SAVE_DIR) / file_name)

        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ–‡ä»¶...")
        dl_resp = session.get(file_url, timeout=120, stream=True)
        with open(save_path, 'wb') as f:
            for chunk in dl_resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        print(f"âœ… æ–‡ä»¶å·²ä¿å­˜åˆ°: {save_path}")

        # è§£æExcel
        print(f"\nğŸ“„ å¼€å§‹è§£æExcelæ–‡ä»¶")
        df = pd.read_excel(save_path, header=None)
        print(f"âœ… è¯»å–æˆåŠŸï¼Œå…± {len(df)} è¡Œï¼Œ{len(df.columns)} åˆ—")
        data_list = []
        skip_count = 0
        coupon_type_skip_count = 0
        for idx in range(2, len(df)):
            row = df.iloc[idx]
            data = kewen_parse_excel_row(row)
            # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºè¡Œ
            if kewen_is_empty_row(data):
                skip_count += 1
                continue
            # æ£€æŸ¥ä¼˜æƒ ç ç±»å‹ï¼Œåªä¿ç•™"å…¨éƒ¨ç "
            if not kewen_is_valid_coupon_type(data):
                coupon_type_skip_count += 1
                continue
            data_list.append(data)
        print(f"âœ… è§£æå®Œæˆ:")
        print(f"   æœ‰æ•ˆæ•°æ®: {len(data_list)} æ¡ (å…¨éƒ¨ç )")
        print(f"   è·³è¿‡ç©ºè¡Œ: {skip_count} æ¡")
        print(f"   è·³è¿‡å…¶ä»–ç ç±»å‹: {coupon_type_skip_count} æ¡ (é—¨åº—ç /å•†å“ç /èŒäººç /å“ç‰Œç )")

        # ä¸Šä¼ æ•°æ®
        print(f"\nğŸ“¤ å¼€å§‹ä¸Šä¼ æ•°æ®åˆ°: {UPLOAD_APIS[table_name]}")
        success_count = 0
        fail_count = 0
        shop_record_counts = {}

        for idx, data in enumerate(data_list, 1):
            try:
                print(f"\n   [{idx}/{len(data_list)}] ä¸Šä¼ æ•°æ®:")
                print(f"      shop_id={data.get('shop_id')}, report_date={data.get('report_date')}, shop_name={data.get('shop_name')}")
                resp = session.post(UPLOAD_APIS[table_name], json=data, headers={'Content-Type': 'application/json'}, timeout=30)
                print(f"      HTTPçŠ¶æ€ç : {resp.status_code}")
                print(f"      å“åº”: {resp.text[:200] if resp.text else '(ç©º)'}")
                if resp.status_code in [200, 201]:
                    success_count += 1
                    shop_id = int(data.get('shop_id', 0))
                    shop_record_counts[shop_id] = shop_record_counts.get(shop_id, 0) + 1
                    print(f"      âœ… æˆåŠŸ")
                else:
                    fail_count += 1
                    print(f"      âŒ å¤±è´¥")
            except Exception as e:
                fail_count += 1
                print(f"      âŒ å¼‚å¸¸: {e}")

        print(f"\nâœ… ä¸Šä¼ å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")

        if fail_count == 0:
            result["success"] = True
            result["record_count"] = success_count
            for shop_id, count in shop_record_counts.items():
                log_success(account_name, shop_id, table_name, start_date, end_date, count)
        else:
            result["error_message"] = f"éƒ¨åˆ†ä¸Šä¼ å¤±è´¥: æˆåŠŸ{success_count}, å¤±è´¥{fail_count}"
            for shop_id in shop_ids:
                log_failure(account_name, shop_id, table_name, start_date, end_date, result["error_message"])

    except Exception as e:
        result["error_message"] = str(e)
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        log_failure(account_name, 0, table_name, start_date, end_date, str(e))

    return result


# ============================================================================
# promotion_daily_report ä»»åŠ¡
# ============================================================================
def run_promotion_daily_report(account_name: str, start_date: str, end_date: str) -> Dict[str, Any]:
    """æ‰§è¡Œpromotion_daily_reportä»»åŠ¡"""
    table_name = "promotion_daily_report"
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š {table_name}")
    print(f"{'=' * 60}")

    result = {"task_name": table_name, "success": False, "record_count": 0, "error_message": "æ— "}

    try:
        disable_proxy()
        Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)

        api_data = load_cookies_from_api(account_name)
        cookies = api_data['cookies']
        mtgsig = api_data['mtgsig']
        shop_info = api_data['shop_info']
        shop_ids = get_shop_ids(shop_info)
        year = start_date.split('-')[0]

        headers = {
            'Accept': '*/*',
            'Referer': 'https://e.dianping.com/shopdiy-node/report',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'X-Requested-With': 'XMLHttpRequest'
        }

        session = get_session()

        # è¯·æ±‚ä¸‹è½½æŠ¥è¡¨
        print(f"\nğŸ” æ­£åœ¨è¯·æ±‚ç”Ÿæˆé—¨åº—æ•°æ®æŠ¥è¡¨...")
        url = "https://e.dianping.com/shopdiy/report/datareport/pc/ajax/downloadReport"

        begin_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        days_diff = (end_dt - begin_dt).days
        compare_end_dt = begin_dt - timedelta(days=1)
        compare_begin_dt = compare_end_dt - timedelta(days=days_diff)

        params = {
            'shopIds': '0', 'launchIds': '0', 'launchPremiumIds': '0', 'planIds': '0',
            'objectUnit': '', 'groupUnit': 'shopId', 'platform': '0',
            'beginDate': start_date, 'endDate': end_date, 'timeUnit': 'day', 'compareEnabled': '0',
            'compareBeginDate': compare_begin_dt.strftime('%Y-%m-%d'),
            'compareEndDate': compare_end_dt.strftime('%Y-%m-%d'),
            'tabIds': 'T30001,T30002,T30003,T30004,T30005,T30048,T30020,T30029,T30006,T30007,T30013,T30014,T30009,T30012,T30011',
            'yodaReady': 'h5', 'csecplatform': '4', 'csecversion': '4.0.4',
            'mtgsig': generate_mtgsig(cookies, mtgsig)
        }

        response = session.get(url, params=params, headers=headers, cookies=cookies, timeout=60)
        resp_json = response.json()
        print(f"ğŸ“Š è¯·æ±‚å“åº”: {resp_json}")
        random_delay()  # åçˆ¬è™«ç­‰å¾…

        # æ£€æŸ¥æ˜¯å¦ç›´æ¥è¿”å›URL
        file_url = None
        if resp_json.get('code') == 200:
            msg = resp_json.get('msg', {})
            if isinstance(msg, dict) and 'S3Url' in msg:
                s3_url = msg.get('S3Url')
                if isinstance(s3_url, list) and s3_url:
                    file_url = s3_url[0]
                elif isinstance(s3_url, str):
                    file_url = s3_url

        # å¦‚æœæ²¡æœ‰ç›´æ¥è¿”å›URLï¼Œç­‰å¾…ä¸‹è½½å†å²
        if not file_url:
            print(f"\nâ³ ç­‰å¾…æŠ¥è¡¨ç”Ÿæˆ...")
            history_url = "https://e.dianping.com/shopdiy/report/datareport/subAccount/common/queryDownloadHistory"

            for _ in range(60):
                time.sleep(5)
                hist_params = {'types': '3,9,10', 'beginDate': '', 'endDate': '', 'pageNum': 1, 'pageSize': 20,
                               'yodaReady': 'h5', 'csecplatform': '4', 'csecversion': '4.0.4', 'mtgsig': generate_mtgsig(cookies, mtgsig)}
                hist_resp = session.get(history_url, params=hist_params, headers=headers, cookies=cookies, timeout=30)
                hist_data = hist_resp.json()

                for record in hist_data.get('records', []):
                    if record.get('status') == 2:
                        file_path = record.get('filePath', '')
                        if isinstance(file_path, list) and file_path:
                            file_url = file_path[0]
                        elif isinstance(file_path, str):
                            file_url = file_path
                        if file_url:
                            print(f"   âœ… æŠ¥è¡¨å·²å°±ç»ª")
                            break
                if file_url:
                    break

        if not file_url:
            raise Exception("æŠ¥è¡¨ç”Ÿæˆè¶…æ—¶")

        random_delay()  # åçˆ¬è™«ç­‰å¾…

        # ä¸‹è½½æ–‡ä»¶
        file_name = f'é—¨åº—æŠ¥è¡¨_{start_date.replace("-", "")}_{end_date.replace("-", "")}.xlsx'
        save_path = str(Path(SAVE_DIR) / file_name)

        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ–‡ä»¶...")
        dl_resp = session.get(file_url, timeout=120, stream=True)
        with open(save_path, 'wb') as f:
            for chunk in dl_resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        print(f"âœ… æ–‡ä»¶å·²ä¿å­˜åˆ°: {save_path}")

        # ä¸Šä¼ æ•°æ®
        print(f"\nğŸ“¤ å¼€å§‹ä¸Šä¼ æŠ¥è¡¨æ•°æ®åˆ°: {UPLOAD_APIS[table_name]}")
        df = pd.read_excel(save_path)
        print(f"   Excelè¡Œæ•°: {len(df)}")
        success_count = 0
        fail_count = 0
        shop_ids_uploaded = set()

        def parse_value(val, default=0):
            if pd.isna(val) or val == '/' or val == '-' or val == '':
                return default
            try:
                return float(val) if '.' in str(val) else int(val)
            except:
                return default

        def format_date(date_str):
            if '-' in str(date_str):
                parts = str(date_str).split('-')
                if len(parts) == 2:
                    return f"{year}-{parts[0].zfill(2)}-{parts[1].zfill(2)}"
            return str(date_str)

        for idx, row in df.iterrows():
            try:
                json_param = {
                    "report_date": format_date(row['æ—¥æœŸ']),
                    "shop_id": int(row['é—¨åº—ID']),
                    "shop_name": str(row['æ¨å¹¿é—¨åº—']),
                    "city_name": str(row['é—¨åº—æ‰€åœ¨åŸå¸‚']),
                    "cost": parse_value(row['èŠ±è´¹ï¼ˆå…ƒï¼‰'], 0.0),
                    "exposure_count": parse_value(row['æ›å…‰ï¼ˆæ¬¡ï¼‰']),
                    "click_count": parse_value(row['ç‚¹å‡»ï¼ˆæ¬¡ï¼‰']),
                    "click_avg_price": parse_value(row['ç‚¹å‡»å‡ä»·ï¼ˆå…ƒï¼‰'], 0.0),
                    "shop_view_count": parse_value(row['å•†æˆ·æµè§ˆé‡ï¼ˆæ¬¡ï¼‰']),
                    "coupon_order_count": parse_value(row['ä¼˜æƒ é¢„è®¢è®¢å•é‡ï¼ˆä¸ªï¼‰']),
                    "groupbuy_order_count": parse_value(row['å›¢è´­è®¢å•é‡ï¼ˆä¸ªï¼‰']),
                    "order_count": parse_value(row['è®¢å•é‡ï¼ˆä¸ªï¼‰']),
                    "view_pic_count": parse_value(row['æŸ¥çœ‹å›¾ç‰‡ï¼ˆæ¬¡ï¼‰']),
                    "view_comment_count": parse_value(row['æŸ¥çœ‹è¯„è®ºï¼ˆæ¬¡ï¼‰']),
                    "view_address_count": parse_value(row['æŸ¥çœ‹åœ°å€ï¼ˆæ¬¡ï¼‰']),
                    "view_phone_count": parse_value(row['æŸ¥çœ‹ç”µè¯ï¼ˆæ¬¡ï¼‰']),
                    "view_groupbuy_count": parse_value(row['æŸ¥çœ‹å›¢è´­ï¼ˆæ¬¡ï¼‰']),
                    "collect_count": parse_value(row['æ”¶è—ï¼ˆæ¬¡ï¼‰']),
                    "share_count": parse_value(row['åˆ†äº«ï¼ˆæ¬¡ï¼‰'])
                }
                print(f"\n   [{idx+1}/{len(df)}] ä¸Šä¼ æ•°æ®:")
                print(f"      shop_id={json_param['shop_id']}, report_date={json_param['report_date']}, shop_name={json_param['shop_name']}")
                resp = requests.post(UPLOAD_APIS[table_name], headers={'Content-Type': 'application/json'},
                                     data=json.dumps(json_param), proxies={'http': None, 'https': None})
                print(f"      HTTPçŠ¶æ€ç : {resp.status_code}")
                print(f"      å“åº”: {resp.text[:200] if resp.text else '(ç©º)'}")
                if resp.status_code == 200:
                    success_count += 1
                    shop_ids_uploaded.add(json_param['shop_id'])
                    print(f"      âœ… æˆåŠŸ")
                else:
                    fail_count += 1
                    print(f"      âŒ å¤±è´¥")
            except Exception as e:
                fail_count += 1
                print(f"      âŒ å¼‚å¸¸: {e}")

        print(f"\nâœ… ä¸Šä¼ å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")

        if fail_count == 0:
            result["success"] = True
            result["record_count"] = success_count
            for shop_id in shop_ids_uploaded:
                log_success(account_name, shop_id, table_name, start_date, end_date, success_count // len(shop_ids_uploaded) if shop_ids_uploaded else success_count)
        else:
            result["error_message"] = f"éƒ¨åˆ†ä¸Šä¼ å¤±è´¥: æˆåŠŸ{success_count}, å¤±è´¥{fail_count}"
            for shop_id in shop_ids:
                log_failure(account_name, shop_id, table_name, start_date, end_date, result["error_message"])

    except Exception as e:
        result["error_message"] = str(e)
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        log_failure(account_name, 0, table_name, start_date, end_date, str(e))

    return result


# ============================================================================
# review_detail_dianping ä»»åŠ¡
# ============================================================================
def run_review_detail_dianping(account_name: str, start_date: str, end_date: str) -> Dict[str, Any]:
    """æ‰§è¡Œreview_detail_dianpingä»»åŠ¡"""
    table_name = "review_detail_dianping"
    print(f"\n{'=' * 60}")
    print(f"ğŸ’¬ {table_name}")
    print(f"{'=' * 60}")

    result = {"task_name": table_name, "success": False, "record_count": 0, "error_message": "æ— "}

    try:
        disable_proxy()
        api_data = load_cookies_from_api(account_name)
        cookies = api_data['cookies']
        mtgsig = api_data['mtgsig']
        shop_info = api_data['shop_info']
        shop_ids = get_shop_ids(shop_info)

        headers = {
            'Accept': 'application/json, text/plain, */*',
            'Referer': 'https://e.dianping.com/vg-platform-reviewmanage/shop-comment-dp/index.html',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        session = get_session()

        def timestamp_to_datetime(ts, default="1997-12-08 00:00:00"):
            if not ts or ts == 0:
                return default
            try:
                return datetime.fromtimestamp(ts / 1000).strftime('%Y-%m-%d %H:%M:%S')
            except:
                return default

        def safe_int(val, default=0):
            if val is None or val == '':
                return default
            try:
                return int(val)
            except:
                return default

        def safe_float(val, default=0.0):
            if val is None or val == '':
                return default
            try:
                return float(val)
            except:
                return default

        def safe_str(val, default=''):
            return str(val) if val is not None else default

        all_reviews = []
        upload_stats = {"success": 0, "failed": 0}
        shop_ids_found = set()
        page_no = 1

        while True:
            print(f"\nğŸ“¡ è·å–ç‚¹è¯„è¯„ä»·æ•°æ® ç¬¬{page_no}é¡µ...")
            url = "https://e.dianping.com/review/app/index/ajax/pcreview/listV2"
            params = {
                'platform': 0, 'shopIdStr': '0', 'tagId': 0,
                'startDate': start_date, 'endDate': end_date,
                'pageNo': page_no, 'pageSize': 50, 'referType': 0, 'category': 0,
                'yodaReady': 'h5', 'csecplatform': '4', 'csecversion': '4.1.1',
                'mtgsig': generate_mtgsig(cookies, mtgsig)
            }
            print(f"   è¯·æ±‚å‚æ•°: platform=0, startDate={start_date}, endDate={end_date}")

            resp = session.get(url, params=params, headers=headers, cookies=cookies, timeout=60, proxies={'http': None, 'https': None})
            resp_json = resp.json()

            print(f"   APIå“åº”ç : {resp_json.get('code')}")
            if resp_json.get('code') != 200:
                print(f"   âŒ APIè¿”å›é”™è¯¯: {resp_json}")
                break

            msg_data = resp_json.get('msg', {})
            reviews = msg_data.get('reviewDetailDTOs', [])
            total = msg_data.get('totalReivewNum', 0)

            print(f"   è·å–åˆ° {len(reviews)} æ¡, æ€»æ•° {total}")
            if not reviews:
                print(f"   âš ï¸ è¯¥æ—¥æœŸèŒƒå›´å†…æ²¡æœ‰ç‚¹è¯„è¯„ä»·æ•°æ®")
                break

            for review in reviews:
                shop_id = safe_int(review.get('shopId'), 0)
                if shop_id:
                    shop_ids_found.add(shop_id)

                # æ˜ å°„æ•°æ®
                star_raw = safe_int(review.get('star'), 0)
                add_time = timestamp_to_datetime(review.get('addTime'))
                update_time = timestamp_to_datetime(review.get('updateTime'), add_time)
                edit_time = timestamp_to_datetime(review.get('editTime'), "1997-12-08 00:00:00")
                score_map = review.get('scoreMap', {}) or {}
                pic_info = review.get('picInfo', []) or []
                video_info = review.get('videoInfo', []) or []
                reply_list = review.get('reviewFollowNoteDtoList', []) or []

                shop_reply = ""
                shop_reply_time = "1997-12-08 00:00:00"
                reply_list_formatted = []
                for reply in reply_list:
                    reply_content = safe_str(reply.get('noteBody', ''))
                    reply_time_str = timestamp_to_datetime(reply.get('addDate', 0), "1997-12-08 00:00:00")
                    reply_list_formatted.append({"reply_time": reply_time_str, "reply_content": reply_content})
                    if not shop_reply:
                        shop_reply = reply_content
                        shop_reply_time = reply_time_str

                upload_data = {
                    "review_id": safe_str(review.get('reviewId'), f"DP_{int(time.time())}"),
                    "shop_id": shop_id,
                    "shop_name": safe_str(review.get('shopName'), 'æœªçŸ¥é—¨åº—'),
                    "city_name": safe_str(review.get('cityName'), 'æœªçŸ¥'),
                    "city_id": safe_int(review.get('cityId'), 0),
                    "user_id": safe_str(review.get('userId'), '0'),
                    "user_nickname": safe_str(review.get('userNickName'), 'åŒ¿åç”¨æˆ·'),
                    "user_face": safe_str(review.get('userFace'), ''),
                    "user_power": safe_str(review.get('userPower'), '') or 'æ™®é€šç”¨æˆ·',
                    "vip_level": safe_int(review.get('vipLevel'), 0),
                    "add_time": add_time,
                    "update_time": update_time,
                    "edit_time": edit_time,
                    "star": star_raw,
                    "star_display": star_raw // 10 if star_raw else 0,
                    "accurate_star": safe_int(review.get('accurateStar'), star_raw),
                    "content": safe_str(review.get('content'), '') or 'æ— ',
                    "score_technician": safe_float(score_map.get('æŠ€å¸ˆ', 0)),
                    "score_service": safe_float(score_map.get('æœåŠ¡', 0)),
                    "score_environment": safe_float(score_map.get('ç¯å¢ƒ', 0)),
                    "score_map": json.dumps(score_map, ensure_ascii=False),
                    "pic_count": len(pic_info),
                    "video_count": len(video_info),
                    "pic_info": json.dumps(pic_info, ensure_ascii=False),
                    "video_info": json.dumps(video_info, ensure_ascii=False),
                    "shop_reply": shop_reply or 'æš‚æ— å›å¤',
                    "shop_reply_time": shop_reply_time,
                    "is_reply_with_photo": safe_int(review.get('isReplyWithPhoto'), 0),
                    "reply_list": json.dumps(reply_list_formatted, ensure_ascii=False),
                    "order_id": safe_int(review.get('orderId'), 0),
                    "deal_group_id": safe_int(review.get('dealGroupId'), 0),
                    "refer_type": safe_int(review.get('referType'), 0),
                    "avg_price": safe_float(review.get('avgPrice'), 0),
                    "serial_numbers": safe_str(review.get('serialNumbers'), '') or 'æ— ',
                    "total_cost": safe_float(review.get('totalCost'), 0),
                    "consume_date": review.get('consumeDate') or "1997-12-08",
                    "status": safe_int(review.get('status'), 1),
                    "quality_score": safe_int(review.get('qualityScore'), 0),
                    "case_status": safe_int(review.get('caseStatus'), 0),
                    "case_status_desc": safe_str(review.get('caseStatusDesc'), ''),
                    "report_status": safe_int(review.get('reportStatus'), 0),
                    "report_status_desc": safe_str(review.get('reportStatusDesc'), '') or 'æ— ',
                    "case_id": safe_int(review.get('caseId'), 0),
                    "show_deal": 1 if review.get('showDeal', True) else 0,
                    "raw_data": json.dumps(review, ensure_ascii=False)
                }

                try:
                    print(f"\n      ä¸Šä¼ ç‚¹è¯„è¯„ä»· review_id={upload_data.get('review_id')}, shop_id={upload_data.get('shop_id')}")
                    print(f"         user_nickname={upload_data.get('user_nickname')}, content={upload_data.get('content', '')[:50]}...")
                    upload_resp = session.post(UPLOAD_APIS[table_name], headers={'Content-Type': 'application/json'},
                                               json=upload_data, timeout=30, proxies={'http': None, 'https': None})
                    print(f"         HTTPçŠ¶æ€ç : {upload_resp.status_code}")
                    print(f"         å“åº”: {upload_resp.text[:200] if upload_resp.text else '(ç©º)'}")
                    if upload_resp.status_code == 200:
                        upload_stats["success"] += 1
                        print(f"         âœ… æˆåŠŸ")
                    else:
                        upload_stats["failed"] += 1
                        print(f"         âŒ å¤±è´¥")
                        print(f"         åŸå§‹æ•°æ®: {json.dumps(review, ensure_ascii=False)[:500]}")
                except Exception as e:
                    upload_stats["failed"] += 1
                    print(f"         âŒ å¼‚å¸¸: {e}")
                    print(f"         åŸå§‹æ•°æ®: {json.dumps(review, ensure_ascii=False)[:500]}")
                time.sleep(0.3)

            all_reviews.extend(reviews)
            if len(all_reviews) >= total:
                break
            page_no += 1
            random_delay()  # åçˆ¬è™«ç­‰å¾…

        print(f"\nğŸ“Š ç‚¹è¯„è¯„ä»·å®Œæˆ: è·å– {len(all_reviews)} æ¡, ä¸Šä¼ æˆåŠŸ {upload_stats['success']}, å¤±è´¥ {upload_stats['failed']}")

        if upload_stats["failed"] == 0:
            result["success"] = True
            result["record_count"] = upload_stats["success"]
            for shop_id in (shop_ids_found or shop_ids):
                log_success(account_name, shop_id, table_name, start_date, end_date, upload_stats["success"])
        else:
            result["error_message"] = f"éƒ¨åˆ†ä¸Šä¼ å¤±è´¥: æˆåŠŸ{upload_stats['success']}, å¤±è´¥{upload_stats['failed']}"
            for shop_id in shop_ids:
                log_failure(account_name, shop_id, table_name, start_date, end_date, result["error_message"])

    except Exception as e:
        result["error_message"] = str(e)
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        log_failure(account_name, 0, table_name, start_date, end_date, str(e))

    return result


# ============================================================================
# review_detail_meituan ä»»åŠ¡
# ============================================================================
def run_review_detail_meituan(account_name: str, start_date: str, end_date: str) -> Dict[str, Any]:
    """æ‰§è¡Œreview_detail_meituanä»»åŠ¡"""
    table_name = "review_detail_meituan"
    print(f"\n{'=' * 60}")
    print(f"ğŸ” {table_name}")
    print(f"{'=' * 60}")

    result = {"task_name": table_name, "success": False, "record_count": 0, "error_message": "æ— "}

    try:
        disable_proxy()
        api_data = load_cookies_from_api(account_name)
        cookies = api_data['cookies']
        mtgsig = api_data['mtgsig']
        shop_info = api_data['shop_info']
        shop_ids = get_shop_ids(shop_info)

        headers = {
            'Accept': 'application/json, text/plain, */*',
            'Referer': 'https://e.dianping.com/vg-platform-reviewmanage/shop-comment-mt/index.html',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        session = get_session()

        def timestamp_to_datetime(ts, default="1997-12-08 00:00:00"):
            if not ts or ts == 0:
                return default
            try:
                return datetime.fromtimestamp(ts / 1000).strftime('%Y-%m-%d %H:%M:%S')
            except:
                return default

        def safe_int(val, default=0):
            if val is None or val == '':
                return default
            try:
                return int(val)
            except:
                return default

        def safe_float(val, default=0.0):
            if val is None or val == '':
                return default
            try:
                return float(val)
            except:
                return default

        def safe_str(val, default=''):
            return str(val) if val is not None else default

        def extract_order_info(order_info_list, field_id, default=''):
            if not order_info_list:
                return default
            for item in order_info_list:
                if item.get('id') == field_id:
                    return safe_str(item.get('content'), default)
            return default

        all_reviews = []
        upload_stats = {"success": 0, "failed": 0}
        shop_ids_found = set()
        page_no = 1

        while True:
            print(f"\nğŸ“¡ è·å–ç¾å›¢è¯„ä»·æ•°æ® ç¬¬{page_no}é¡µ...")
            url = "https://e.dianping.com/review/app/index/ajax/pcreview/listV2"
            params = {
                'platform': 1, 'shopIdStr': '0', 'tagId': 0,
                'startDate': start_date, 'endDate': end_date,
                'pageNo': page_no, 'pageSize': 50, 'referType': 0, 'category': 0,
                'yodaReady': 'h5', 'csecplatform': '4', 'csecversion': '4.1.1',
                'mtgsig': generate_mtgsig(cookies, mtgsig)
            }
            print(f"   è¯·æ±‚å‚æ•°: platform=1, startDate={start_date}, endDate={end_date}")

            resp = session.get(url, params=params, headers=headers, cookies=cookies, timeout=60, proxies={'http': None, 'https': None})
            resp_json = resp.json()

            print(f"   APIå“åº”ç : {resp_json.get('code')}")
            if resp_json.get('code') != 200:
                print(f"   âŒ APIè¿”å›é”™è¯¯: {resp_json}")
                break

            msg_data = resp_json.get('msg', {})
            reviews = msg_data.get('reviewDetailDTOs', [])
            total = msg_data.get('totalReivewNum', 0)

            print(f"   è·å–åˆ° {len(reviews)} æ¡, æ€»æ•° {total}")
            if not reviews:
                print(f"   âš ï¸ è¯¥æ—¥æœŸèŒƒå›´å†…æ²¡æœ‰ç¾å›¢è¯„ä»·æ•°æ®")
                break

            for review in reviews:
                shop_id = safe_int(review.get('shopId'), 0)
                if shop_id:
                    shop_ids_found.add(shop_id)

                star_raw = safe_int(review.get('star'), 0)
                add_time = timestamp_to_datetime(review.get('addTime'))
                update_time = timestamp_to_datetime(review.get('updateTime'), add_time)
                edit_time = timestamp_to_datetime(review.get('editTime'), "1997-12-08 00:00:00")
                pic_info = review.get('picInfo', []) or []
                video_info = review.get('videoInfo', []) or []
                shop_reply = safe_str(review.get('shopReply'), '') or 'æš‚æ— å›å¤'
                shop_reply_time = timestamp_to_datetime(review.get('shopReplyTime'), "1997-12-08 00:00:00")

                reply_list_formatted = []
                if review.get('shopReply'):
                    reply_list_formatted.append({"reply_time": shop_reply_time, "reply_content": review.get('shopReply')})

                order_info_list = review.get('orderInfoDTOList', [])
                business_type = extract_order_info(order_info_list, 9, 'æ— ')
                coupon_code = extract_order_info(order_info_list, 1, 'æ— ')
                product_name = extract_order_info(order_info_list, 2, 'æ— ')
                order_time = extract_order_info(order_info_list, 3, '1997-12-08').strip()
                consume_time = extract_order_info(order_info_list, 4, '1997-12-08').strip()
                quantity = safe_int(extract_order_info(order_info_list, 5, '0'), 0)
                price = safe_float(extract_order_info(order_info_list, 6, '0'), 0)

                upload_data = {
                    "review_id": safe_str(review.get('reviewId'), f"MT_{int(time.time())}"),
                    "feedback_id": safe_int(review.get('feedbackId'), 0),
                    "shop_id": shop_id,
                    "shop_name": safe_str(review.get('shopName'), 'æœªçŸ¥é—¨åº—'),
                    "city_name": safe_str(review.get('cityName'), 'æœªçŸ¥'),
                    "city_id": safe_int(review.get('cityId'), 0),
                    "user_id": safe_str(review.get('userId'), '0'),
                    "user_nickname": safe_str(review.get('userNickName'), 'åŒ¿åç”¨æˆ·'),
                    "user_face": safe_str(review.get('userFace'), ''),
                    "user_power": safe_str(review.get('userPower'), '') or 'æ™®é€šç”¨æˆ·',
                    "anonymous": 1 if review.get('anonymous', False) else 0,
                    "add_time": add_time,
                    "update_time": update_time,
                    "edit_time": edit_time,
                    "star": star_raw,
                    "star_display": star_raw // 10 if star_raw else 0,
                    "accurate_star": safe_int(review.get('accurateStar'), star_raw),
                    "content": safe_str(review.get('content'), '') or 'æ— ',
                    "pic_count": len(pic_info),
                    "video_count": len(video_info),
                    "pic_info": json.dumps(pic_info, ensure_ascii=False),
                    "video_info": json.dumps(video_info, ensure_ascii=False),
                    "shop_reply": shop_reply,
                    "shop_reply_time": shop_reply_time,
                    "reply_list": json.dumps(reply_list_formatted, ensure_ascii=False),
                    "order_id": safe_int(review.get('orderId'), 0),
                    "refer_type": safe_int(review.get('referType'), 0),
                    "business_type": business_type,
                    "coupon_code": coupon_code,
                    "product_name": product_name,
                    "order_time": order_time,
                    "consume_time": consume_time,
                    "quantity": quantity,
                    "price": price,
                    "case_status": safe_int(review.get('caseStatus'), 0),
                    "report_status": safe_int(review.get('reportStatus'), 0),
                    "case_id": safe_int(review.get('caseId'), 0),
                    "show_deal": 1 if review.get('showDeal', True) else 0,
                    "raw_data": json.dumps(review, ensure_ascii=False)
                }

                try:
                    print(f"\n      ä¸Šä¼ ç¾å›¢è¯„ä»· review_id={upload_data.get('review_id')}, shop_id={upload_data.get('shop_id')}")
                    print(f"         user_nickname={upload_data.get('user_nickname')}, content={upload_data.get('content', '')[:50]}...")
                    upload_resp = session.post(UPLOAD_APIS[table_name], headers={'Content-Type': 'application/json'},
                                               json=upload_data, timeout=30, proxies={'http': None, 'https': None})
                    print(f"         HTTPçŠ¶æ€ç : {upload_resp.status_code}")
                    print(f"         å“åº”: {upload_resp.text[:200] if upload_resp.text else '(ç©º)'}")
                    if upload_resp.status_code == 200:
                        upload_stats["success"] += 1
                        print(f"         âœ… æˆåŠŸ")
                    else:
                        upload_stats["failed"] += 1
                        print(f"         âŒ å¤±è´¥")
                        print(f"         åŸå§‹æ•°æ®: {json.dumps(review, ensure_ascii=False)[:500]}")
                except Exception as e:
                    upload_stats["failed"] += 1
                    print(f"         âŒ å¼‚å¸¸: {e}")
                    print(f"         åŸå§‹æ•°æ®: {json.dumps(review, ensure_ascii=False)[:500]}")
                time.sleep(0.3)

            all_reviews.extend(reviews)
            if len(all_reviews) >= total:
                break
            page_no += 1
            random_delay()  # åçˆ¬è™«ç­‰å¾…

        print(f"\nğŸ“Š ç¾å›¢è¯„ä»·å®Œæˆ: è·å– {len(all_reviews)} æ¡, ä¸Šä¼ æˆåŠŸ {upload_stats['success']}, å¤±è´¥ {upload_stats['failed']}")

        if upload_stats["failed"] == 0:
            result["success"] = True
            result["record_count"] = upload_stats["success"]
            for shop_id in (shop_ids_found or shop_ids):
                log_success(account_name, shop_id, table_name, start_date, end_date, upload_stats["success"])
        else:
            result["error_message"] = f"éƒ¨åˆ†ä¸Šä¼ å¤±è´¥: æˆåŠŸ{upload_stats['success']}, å¤±è´¥{upload_stats['failed']}"
            for shop_id in shop_ids:
                log_failure(account_name, shop_id, table_name, start_date, end_date, result["error_message"])

    except Exception as e:
        result["error_message"] = str(e)
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        log_failure(account_name, 0, table_name, start_date, end_date, str(e))

    return result


# ============================================================================
# review_summary_dianping ä»»åŠ¡
# ============================================================================
def run_review_summary_dianping(account_name: str, start_date: str, end_date: str) -> Dict[str, Any]:
    """æ‰§è¡Œreview_summary_dianpingä»»åŠ¡"""
    table_name = "review_summary_dianping"
    print(f"\n{'=' * 60}")
    print(f"ğŸ’¬ {table_name}")
    print(f"{'=' * 60}")

    result = {"task_name": table_name, "success": False, "record_count": 0, "error_message": "æ— "}

    try:
        disable_proxy()
        Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)

        api_data = load_cookies_from_api(account_name)
        cookies = api_data['cookies']
        mtgsig = api_data['mtgsig']
        shop_info = api_data['shop_info']
        shop_ids = get_shop_ids(shop_info)

        headers = {
            'Accept': 'application/json, text/plain, */*',
            'Content-Type': 'application/json',
            'Origin': 'https://e.dianping.com',
            'Referer': 'https://e.dianping.com/app/merchant-workbench/index.html',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        session = get_session()

        # è§¦å‘ä¸‹è½½
        print(f"\nğŸ“¤ è§¦å‘ä¸‹è½½ä»»åŠ¡...")
        trigger_url = "https://e.dianping.com/gateway/merchant/review/pc/reviewdownload"
        trigger_params = {"yodaReady": "h5", "csecplatform": "4", "csecversion": "4.1.1", "mtgsig": generate_mtgsig(cookies, mtgsig)}
        trigger_payload = {"tagId": 0, "platform": 1, "shopIdStr": "0", "startDate": start_date, "endDate": end_date}

        trigger_resp = session.post(trigger_url, params=trigger_params, headers=headers, cookies=cookies, json=trigger_payload, timeout=60)
        print(f"   å“åº”: {trigger_resp.json()}")
        random_delay()  # åçˆ¬è™«ç­‰å¾…

        # ç­‰å¾…æ–‡ä»¶ç”Ÿæˆ
        print(f"\nâ³ ç­‰å¾…æ–‡ä»¶ç”Ÿæˆ...")
        trigger_time = time.time()
        file_record = None

        for _ in range(30):
            time.sleep(2)
            list_url = "https://e.dianping.com/gateway/merchant/downloadcenter/list"
            list_params = {'pageNo': 1, 'pageSize': 20, 'yodaReady': 'h5', 'csecplatform': '4', 'csecversion': '4.1.1', 'mtgsig': generate_mtgsig(cookies, mtgsig)}
            list_resp = session.get(list_url, params=list_params, headers=headers, cookies=cookies, timeout=30)
            list_data = list_resp.json()

            if list_data.get('code') == 200:
                for record in list_data.get('data', {}).get('records', []):
                    file_name = record.get('fileName', '')
                    if 'é—¨åº—è¯„ä»·' in file_name and record.get('recordStatus') == 300 and record.get('downloadable') == "1" and record.get('fileUrl'):
                        add_time = record.get('addTime', '')
                        try:
                            file_time = datetime.strptime(add_time, '%Y-%m-%d %H:%M:%S')
                            if file_time.timestamp() >= trigger_time - 10:
                                file_record = record
                                print(f"   âœ… æ–‡ä»¶å·²å°±ç»ª: {file_name}")
                                break
                        except:
                            file_record = record
                            break
            if file_record:
                break

        if not file_record:
            raise Exception("æ–‡ä»¶ç”Ÿæˆè¶…æ—¶")

        random_delay()  # åçˆ¬è™«ç­‰å¾…

        # ä¸‹è½½æ–‡ä»¶
        file_url = file_record['fileUrl']
        file_name = file_record.get('fileName', f'ç‚¹è¯„è¯„ä»·_{start_date}_{end_date}.xlsx')
        save_path = str(Path(SAVE_DIR) / file_name)

        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ–‡ä»¶...")
        print(f"   URL: {file_url[:80]}...")
        dl_resp = session.get(file_url, timeout=120, stream=True)
        with open(save_path, 'wb') as f:
            for chunk in dl_resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        file_size = Path(save_path).stat().st_size
        print(f"âœ… æ–‡ä»¶å·²ä¿å­˜åˆ°: {save_path}")
        print(f"   æ–‡ä»¶å¤§å°: {file_size / 1024:.2f} KB")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
        if file_size < 1000:  # å°äº1KBå¯èƒ½æ˜¯ç©ºæ–‡ä»¶
            print(f"âš ï¸ æ–‡ä»¶å¯èƒ½ä¸ºç©ºæˆ–æ— æ•ˆ (å¤§å°: {file_size} å­—èŠ‚)")

        # ä¸Šä¼ æ•°æ®
        print(f"\nğŸ“¤ å¼€å§‹ä¸Šä¼ è¯„ä»·æ•°æ®...")
        try:
            df = pd.read_excel(save_path)
        except ValueError as e:
            if "Worksheet index" in str(e) or "0 worksheets found" in str(e):
                print(f"âš ï¸ Excelæ–‡ä»¶ä¸ºç©º(æ²¡æœ‰å·¥ä½œè¡¨)ï¼Œè¯¥æ—¥æœŸèŒƒå›´å¯èƒ½æ²¡æœ‰ç‚¹è¯„è¯„ä»·æ•°æ®")
                result["success"] = True
                result["record_count"] = 0
                result["error_message"] = "æ— æ•°æ®"
                return result
            raise
        success_count = 0
        fail_count = 0
        shop_ids_found = set()

        EMPTY_DATETIME = "1970-01-01 00:00:00"

        def format_datetime(dt_str):
            if pd.isna(dt_str) or str(dt_str).strip() == '':
                return EMPTY_DATETIME
            dt_str = str(dt_str).strip()
            for fmt in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M:%S", "%Y/%m/%d %H:%M"]:
                try:
                    return datetime.strptime(dt_str, fmt).strftime("%Y-%m-%d %H:%M:%S")
                except:
                    continue
            return dt_str

        def safe_int(val, default=0):
            if pd.isna(val):
                return default
            try:
                return int(val)
            except:
                return default

        def safe_str(val, default=""):
            if pd.isna(val):
                return default
            return str(val).strip()

        for idx, row in df.iterrows():
            try:
                content = safe_str(row.get('è¯„ä»·å†…å®¹')) or 'æ— '
                is_replied = "æ˜¯" if row.get('å•†å®¶æ˜¯å¦å·²ç»å›å¤') == 'å·²å›å¤' else "å¦"
                is_after_consume = "æ˜¯" if row.get('æ˜¯å¦æ¶ˆè´¹åè¯„ä»·') == 'æ˜¯' else "å¦"
                dp_shop_id = safe_int(row.get('ç‚¹è¯„é—¨åº—ID'), None)
                if dp_shop_id:
                    shop_ids_found.add(dp_shop_id)

                params = {
                    "review_time": format_datetime(row.get('è¯„ä»·æ—¶é—´')),
                    "city": safe_str(row.get('åŸå¸‚')),
                    "shop_name": safe_str(row.get('è¯„ä»·é—¨åº—')),
                    "dianping_shop_id": dp_shop_id,
                    "meituan_shop_id": safe_int(row.get('ç¾å›¢é—¨åº—ID'), None),
                    "user_nickname": safe_str(row.get('ç”¨æˆ·æ˜µç§°')),
                    "star": safe_str(row.get('æ˜Ÿçº§')),
                    "score_detail": safe_str(row.get('è¯„åˆ†')),
                    "content": content,
                    "content_length": safe_int(row.get('è¯„ä»·æ­£æ–‡å­—æ•°'), len(content)),
                    "pic_count": safe_int(row.get('å›¾ç‰‡æ•°'), 0),
                    "video_count": safe_int(row.get('è§†é¢‘æ•°'), 0),
                    "is_replied": is_replied,
                    "first_reply_time": format_datetime(row.get('å•†å®¶é¦–æ¬¡å›å¤æ—¶é—´')),
                    "is_after_consume": is_after_consume,
                    "consume_time": format_datetime(row.get('æ¶ˆè´¹æ—¶é—´'))
                }

                print(f"\n   [{idx+1}/{len(df)}] ä¸Šä¼ ç‚¹è¯„è¯„ä»·:")
                print(f"      shop_name={params.get('shop_name')}, dianping_shop_id={params.get('dianping_shop_id')}")
                print(f"      user_nickname={params.get('user_nickname')}, content={params.get('content', '')[:50]}...")
                resp = requests.post(UPLOAD_APIS[table_name], headers={'Content-Type': 'application/json'},
                                     data=json.dumps(params, ensure_ascii=False).encode('utf-8'),
                                     timeout=30, proxies={'http': None, 'https': None})
                print(f"      HTTPçŠ¶æ€ç : {resp.status_code}")
                print(f"      å“åº”: {resp.text[:200] if resp.text else '(ç©º)'}")
                if resp.status_code == 200:
                    success_count += 1
                    print(f"      âœ… æˆåŠŸ")
                else:
                    fail_count += 1
                    print(f"      âŒ å¤±è´¥")
                    print(f"      å®Œæ•´å‚æ•°: {json.dumps(params, ensure_ascii=False)}")
            except Exception as e:
                fail_count += 1
                print(f"      âŒ å¼‚å¸¸: {e}")
                print(f"      å®Œæ•´å‚æ•°: {json.dumps(params, ensure_ascii=False)}")

        print(f"\nâœ… ä¸Šä¼ å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")

        if fail_count == 0:
            result["success"] = True
            result["record_count"] = success_count
            for shop_id in (shop_ids_found or shop_ids):
                log_success(account_name, shop_id, table_name, start_date, end_date, success_count)
        else:
            result["error_message"] = f"éƒ¨åˆ†ä¸Šä¼ å¤±è´¥: æˆåŠŸ{success_count}, å¤±è´¥{fail_count}"
            for shop_id in shop_ids:
                log_failure(account_name, shop_id, table_name, start_date, end_date, result["error_message"])

    except Exception as e:
        result["error_message"] = str(e)
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        log_failure(account_name, 0, table_name, start_date, end_date, str(e))

    return result


# ============================================================================
# review_summary_meituan ä»»åŠ¡
# ============================================================================
def run_review_summary_meituan(account_name: str, start_date: str, end_date: str) -> Dict[str, Any]:
    """æ‰§è¡Œreview_summary_meituanä»»åŠ¡"""
    table_name = "review_summary_meituan"
    print(f"\n{'=' * 60}")
    print(f"ğŸ” {table_name}")
    print(f"{'=' * 60}")

    result = {"task_name": table_name, "success": False, "record_count": 0, "error_message": "æ— "}

    try:
        disable_proxy()
        Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)

        api_data = load_cookies_from_api(account_name)
        cookies = api_data['cookies']
        mtgsig = api_data['mtgsig']
        shop_info = api_data['shop_info']
        shop_ids = get_shop_ids(shop_info)

        headers = {
            'Accept': 'application/json, text/plain, */*',
            'Content-Type': 'application/json',
            'Origin': 'https://e.dianping.com',
            'Referer': 'https://e.dianping.com/vg-platform-reviewmanage/shop-comment-mt/index.html',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        session = get_session()

        # è§¦å‘ä¸‹è½½
        print(f"\nğŸ“¤ è§¦å‘ç¾å›¢è¯„ä»·ä¸‹è½½ä»»åŠ¡...")
        trigger_url = "https://e.dianping.com/gateway/merchant/review/pc/reviewdownload"
        trigger_params = {"yodaReady": "h5", "csecplatform": "4", "csecversion": "4.1.1", "mtgsig": generate_mtgsig(cookies, mtgsig)}
        trigger_payload = {"tagId": 0, "platform": 2, "shopIdStr": "0", "startDate": start_date, "endDate": end_date}

        trigger_resp = session.post(trigger_url, params=trigger_params, headers=headers, cookies=cookies, json=trigger_payload, timeout=60)
        print(f"   å“åº”: {trigger_resp.json()}")
        random_delay()  # åçˆ¬è™«ç­‰å¾…

        # ç­‰å¾…æ–‡ä»¶ç”Ÿæˆ
        print(f"\nâ³ ç­‰å¾…æ–‡ä»¶ç”Ÿæˆ...")
        trigger_time = time.time()
        file_record = None

        for _ in range(30):
            time.sleep(2)
            list_url = "https://e.dianping.com/gateway/merchant/downloadcenter/list"
            list_params = {'pageNo': 1, 'pageSize': 20, 'yodaReady': 'h5', 'csecplatform': '4', 'csecversion': '4.1.1', 'mtgsig': generate_mtgsig(cookies, mtgsig)}
            list_resp = session.get(list_url, params=list_params, headers=headers, cookies=cookies, timeout=30)
            list_data = list_resp.json()

            if list_data.get('code') == 200:
                for record in list_data.get('data', {}).get('records', []):
                    file_name = record.get('fileName', '')
                    if ('è¯„ä»·' in file_name or 'é—¨åº—è¯„ä»·' in file_name) and record.get('recordStatus') == 300 and record.get('downloadable') == "1" and record.get('fileUrl'):
                        add_time = record.get('addTime', '')
                        try:
                            file_time = datetime.strptime(add_time, '%Y-%m-%d %H:%M:%S')
                            if file_time.timestamp() >= trigger_time - 10:
                                file_record = record
                                print(f"   âœ… æ–‡ä»¶å·²å°±ç»ª: {file_name}")
                                break
                        except:
                            file_record = record
                            break
            if file_record:
                break

        if not file_record:
            raise Exception("æ–‡ä»¶ç”Ÿæˆè¶…æ—¶")

        random_delay()  # åçˆ¬è™«ç­‰å¾…

        # ä¸‹è½½æ–‡ä»¶
        file_url = file_record['fileUrl']
        file_name = file_record.get('fileName', f'ç¾å›¢è¯„ä»·_{start_date}_{end_date}.xlsx')
        save_path = str(Path(SAVE_DIR) / file_name)

        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ–‡ä»¶...")
        print(f"   URL: {file_url[:80]}...")
        dl_resp = session.get(file_url, timeout=120, stream=True)
        with open(save_path, 'wb') as f:
            for chunk in dl_resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        file_size = Path(save_path).stat().st_size
        print(f"âœ… æ–‡ä»¶å·²ä¿å­˜åˆ°: {save_path}")
        print(f"   æ–‡ä»¶å¤§å°: {file_size / 1024:.2f} KB")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
        if file_size < 1000:
            print(f"âš ï¸ æ–‡ä»¶å¯èƒ½ä¸ºç©ºæˆ–æ— æ•ˆ (å¤§å°: {file_size} å­—èŠ‚)")

        # ä¸Šä¼ æ•°æ®
        print(f"\nğŸ“¤ å¼€å§‹ä¸Šä¼ ç¾å›¢è¯„ä»·æ•°æ®...")
        try:
            df = pd.read_excel(save_path)
        except ValueError as e:
            if "Worksheet index" in str(e) or "0 worksheets found" in str(e):
                print(f"âš ï¸ Excelæ–‡ä»¶ä¸ºç©º(æ²¡æœ‰å·¥ä½œè¡¨)ï¼Œè¯¥æ—¥æœŸèŒƒå›´å¯èƒ½æ²¡æœ‰ç¾å›¢è¯„ä»·æ•°æ®")
                result["success"] = True
                result["record_count"] = 0
                result["error_message"] = "æ— æ•°æ®"
                return result
            raise
        success_count = 0
        fail_count = 0
        shop_ids_found = set()

        EMPTY_DATETIME = "1970-01-01 00:00:00"

        def format_datetime(dt_str):
            if pd.isna(dt_str) or str(dt_str).strip() == '':
                return EMPTY_DATETIME
            dt_str = str(dt_str).strip()
            for fmt in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M:%S", "%Y/%m/%d %H:%M", "%Y-%m-%d", "%Y/%m/%d"]:
                try:
                    dt = datetime.strptime(dt_str, fmt)
                    if fmt in ["%Y-%m-%d", "%Y/%m/%d"]:
                        return dt.strftime("%Y-%m-%d")
                    return dt.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    continue
            return dt_str

        def safe_int(val, default=0):
            if pd.isna(val):
                return default
            try:
                return int(val)
            except:
                return default

        def safe_str(val, default=""):
            if pd.isna(val):
                return default
            return str(val).strip()

        for idx, row in df.iterrows():
            try:
                content = safe_str(row.get('è¯„ä»·å†…å®¹')) or 'æ— '
                is_replied_raw = row.get('å•†å®¶æ˜¯å¦å·²ç»å›å¤')
                is_replied = "æ˜¯" if is_replied_raw == 'å·²å›å¤' or is_replied_raw == 'æ˜¯' else "å¦"
                is_after_consume = "æ˜¯" if row.get('æ˜¯å¦æ¶ˆè´¹åè¯„ä»·') == 'æ˜¯' else "å¦"
                mt_shop_id = safe_int(row.get('ç¾å›¢é—¨åº—ID'), None)
                if mt_shop_id:
                    shop_ids_found.add(mt_shop_id)

                params = {
                    "review_time": format_datetime(row.get('è¯„ä»·æ—¶é—´')),
                    "city": safe_str(row.get('åŸå¸‚')),
                    "shop_name": safe_str(row.get('è¯„ä»·é—¨åº—')),
                    "dianping_shop_id": safe_int(row.get('ç‚¹è¯„é—¨åº—ID'), None),
                    "meituan_shop_id": mt_shop_id,
                    "user_nickname": safe_str(row.get('ç”¨æˆ·æ˜µç§°')),
                    "star": safe_str(row.get('æ˜Ÿçº§')),
                    "content": content,
                    "content_length": safe_int(row.get('è¯„ä»·æ­£æ–‡å­—æ•°'), len(content)),
                    "pic_count": safe_int(row.get('å›¾ç‰‡æ•°'), 0),
                    "video_count": safe_int(row.get('è§†é¢‘æ•°'), 0),
                    "is_replied": is_replied,
                    "first_reply_time": format_datetime(row.get('å•†å®¶é¦–æ¬¡å›å¤æ—¶é—´')),
                    "is_after_consume": is_after_consume,
                    "consume_time": format_datetime(row.get('æ¶ˆè´¹æ—¶é—´'))
                }

                print(f"\n   [{idx+1}/{len(df)}] ä¸Šä¼ ç¾å›¢è¯„ä»·:")
                print(f"      shop_name={params.get('shop_name')}, meituan_shop_id={params.get('meituan_shop_id')}")
                print(f"      user_nickname={params.get('user_nickname')}, content={params.get('content', '')[:50]}...")
                resp = requests.post(UPLOAD_APIS[table_name], headers={'Content-Type': 'application/json'},
                                     data=json.dumps(params, ensure_ascii=False).encode('utf-8'),
                                     timeout=30, proxies={'http': None, 'https': None})
                print(f"      HTTPçŠ¶æ€ç : {resp.status_code}")
                print(f"      å“åº”: {resp.text[:200] if resp.text else '(ç©º)'}")
                if resp.status_code == 200:
                    success_count += 1
                    print(f"      âœ… æˆåŠŸ")
                else:
                    fail_count += 1
                    print(f"      âŒ å¤±è´¥")
                    print(f"      å®Œæ•´å‚æ•°: {json.dumps(params, ensure_ascii=False)}")
            except Exception as e:
                fail_count += 1
                print(f"      âŒ å¼‚å¸¸: {e}")
                print(f"      å®Œæ•´å‚æ•°: {json.dumps(params, ensure_ascii=False)}")

        print(f"\nâœ… ä¸Šä¼ å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")

        if fail_count == 0:
            result["success"] = True
            result["record_count"] = success_count
            for shop_id in (shop_ids_found or shop_ids):
                log_success(account_name, shop_id, table_name, start_date, end_date, success_count)
        else:
            result["error_message"] = f"éƒ¨åˆ†ä¸Šä¼ å¤±è´¥: æˆåŠŸ{success_count}, å¤±è´¥{fail_count}"
            for shop_id in shop_ids:
                log_failure(account_name, shop_id, table_name, start_date, end_date, result["error_message"])

    except Exception as e:
        result["error_message"] = str(e)
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        log_failure(account_name, 0, table_name, start_date, end_date, str(e))

    return result


# ============================================================================
# DianpingStoreStats ç±» (é—¨åº—ç»Ÿè®¡æ•°æ®é‡‡é›†ï¼Œä½¿ç”¨Playwrightæµè§ˆå™¨)
# ============================================================================
class DianpingStoreStats:
    """å¤§ä¼—ç‚¹è¯„é—¨åº—ç»Ÿè®¡æ•°æ®é‡‡é›†ç±»ï¼ˆå¸¦Playwrightæ”¯æŒï¼‰"""

    def __init__(self, account_name: str, platform_api_url: str, headless: bool = True, disable_proxy: bool = True, external_page=None):
        """åˆå§‹åŒ–

        Args:
            account_name: è´¦æˆ·åç§°
            platform_api_url: å¹³å°API URL
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            disable_proxy: æ˜¯å¦ç¦ç”¨ä»£ç†
            external_page: å¤–éƒ¨ä¼ å…¥çš„ Playwright page å¯¹è±¡ï¼ˆç”¨äºé¡µé¢é©±åŠ¨æ¨¡å¼ï¼‰
        """
        self.account_name = account_name
        self.platform_api_url = platform_api_url
        self.headless = headless
        self.disable_proxy = disable_proxy
        self.state_file = os.path.join(STATE_DIR, f'dianping_state_{account_name}.json')

        # Playwrightç›¸å…³
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None

        # å¤–éƒ¨ä¼ å…¥çš„ page å¯¹è±¡ï¼ˆé¡µé¢é©±åŠ¨æ¨¡å¼ä½¿ç”¨ï¼‰
        self.external_page = external_page
        self.use_external_page = external_page is not None

        # ä»APIè·å–çš„æ•°æ®
        self.cookies = {}
        self.mtgsig_from_api = None
        self.shop_id = None
        self.shop_list = []
        self.product_mapping = []
        self.shop_region_info = {}
        self.cookie_data = None

        if self.disable_proxy:
            self._disable_proxy()

        self._load_account_info_from_api()

    def _disable_proxy(self):
        """ç¦ç”¨ç³»ç»Ÿä»£ç†"""
        proxy_vars = [
            'HTTP_PROXY', 'HTTPS_PROXY', 'FTP_PROXY', 'SOCKS_PROXY',
            'http_proxy', 'https_proxy', 'ftp_proxy', 'socks_proxy',
            'ALL_PROXY', 'all_proxy', 'NO_PROXY', 'no_proxy'
        ]
        for var in proxy_vars:
            os.environ.pop(var, None)
        os.environ['NO_PROXY'] = '*'
        os.environ['no_proxy'] = '*'
        print("âœ… å·²ç¦ç”¨ç³»ç»Ÿä»£ç†")

    def _get_session(self) -> requests.Session:
        """è·å–ç¦ç”¨ä»£ç†çš„session"""
        session = requests.Session()
        session.trust_env = False
        session.proxies = {'http': None, 'https': None, 'ftp': None, 'socks': None, 'no_proxy': '*'}
        session.mount('http://', requests.adapters.HTTPAdapter())
        session.mount('https://', requests.adapters.HTTPAdapter())
        return session

    def _load_account_info_from_api(self):
        """ä»APIæ¥å£åŠ è½½è´¦æˆ·ä¿¡æ¯"""
        try:
            print(f"ğŸ” æ­£åœ¨ä»APIè·å–è´¦æˆ· [{self.account_name}] çš„å®Œæ•´ä¿¡æ¯...")
            headers = {'Content-Type': 'application/json'}
            data = json.dumps({"account": self.account_name})

            session = self._get_session()
            response = session.post(self.platform_api_url, headers=headers, data=data, timeout=30)
            response.raise_for_status()
            result = response.json()

            if not result or not result.get('success'):
                raise Exception(f"APIè¿”å›å¤±è´¥")

            data = result.get('data', {})
            if not data:
                raise Exception(f"APIè¿”å›çš„dataä¸ºç©º")

            self.cookie_data = data

            # è·å–cookies
            cookie_data = data.get('cookie', {})
            if cookie_data:
                self.cookies = cookie_data
                print(f"âœ… æˆåŠŸåŠ è½½ {len(self.cookies)} ä¸ªcookies")
            else:
                raise Exception("æœªè·å–åˆ°cookieæ•°æ®")

            # è·å–mtgsig
            mtgsig_data = data.get('mtgsig')
            if mtgsig_data:
                if isinstance(mtgsig_data, str):
                    self.mtgsig_from_api = mtgsig_data
                else:
                    self.mtgsig_from_api = json.dumps(mtgsig_data)
                print(f"   å·²è·å–mtgsig: {self.mtgsig_from_api[:50]}...")

            # è·å–é—¨åº—åˆ—è¡¨
            stores_json = data.get('stores_json', [])
            if stores_json:
                self.shop_list = stores_json
                print(f"âœ… æˆåŠŸåŠ è½½ {len(self.shop_list)} ä¸ªé—¨åº—")
                for shop in self.shop_list:
                    print(f"   - {shop.get('shop_name')} ({shop.get('shop_id')})")
            else:
                raise Exception("æœªè·å–åˆ°é—¨åº—åˆ—è¡¨")

            # è·å–å›¢è´­IDæ˜ å°„
            brands_json = data.get('brands_json', [])
            if brands_json:
                self.product_mapping = brands_json
                print(f"âœ… æˆåŠŸåŠ è½½ {len(self.product_mapping)} ä¸ªå›¢è´­IDæ˜ å°„")

            # è·å–é—¨åº—å•†åœˆä¿¡æ¯
            compare_regions = data.get('compareRegions_json', {})
            if compare_regions:
                self.shop_region_info = compare_regions
                print(f"âœ… æˆåŠŸåŠ è½½ {len(self.shop_region_info)} ä¸ªé—¨åº—å•†åœˆä¿¡æ¯")

            # è·å–åº—é“ºID
            self.shop_id = self.cookies.get('mpmerchant_portal_shopid', '')
            if not self.shop_id and stores_json:
                self.shop_id = stores_json[0].get('shop_id')

        except Exception as e:
            print(f"âŒ åŠ è½½è´¦æˆ·ä¿¡æ¯å¤±è´¥: {e}")
            raise

    def _install_browser(self):
        """è‡ªåŠ¨å®‰è£…Playwrightæµè§ˆå™¨"""
        print("\nâš ï¸ æ£€æµ‹åˆ°Chromiumæµè§ˆå™¨æœªå®‰è£…ï¼Œæ­£åœ¨è‡ªåŠ¨ä¸‹è½½...")
        try:
            process = subprocess.Popen(
                [sys.executable, '-m', 'playwright', 'install', 'chromium'],
                stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1
            )
            for line in process.stdout:
                print(line.strip())
            process.wait()
            return process.returncode == 0
        except Exception as e:
            print(f"å®‰è£…å¤±è´¥: {e}")
            return False

    def _convert_cookies_to_playwright_format(self, cookie_dict: dict) -> list:
        """å°†cookieå­—å…¸è½¬æ¢ä¸ºPlaywrightæ ¼å¼"""
        playwright_cookies = []
        for name, value in cookie_dict.items():
            cookie = {'name': name, 'value': str(value), 'domain': '.dianping.com', 'path': '/'}
            playwright_cookies.append(cookie)
        return playwright_cookies

    def _check_login_status(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¤„äºç™»å½•çŠ¶æ€"""
        try:
            self.page.goto(
                "https://e.dianping.com/app/vg-pc-platform-merchant-selfhelp/newNoticeCenter.html",
                wait_until='networkidle', timeout=15000
            )
            time.sleep(2)
            current_url = self.page.url
            if 'login' in current_url.lower():
                return False
            has_content = self.page.evaluate("() => document.body.textContent.length > 100")
            return has_content
        except Exception as e:
            print(f"âœ— ç™»å½•æ£€æµ‹å¤±è´¥: {e}")
            return False

    def start_browser(self):
        """å¯åŠ¨æµè§ˆå™¨å¹¶ç™»å½•"""
        # å¦‚æœä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ pageï¼Œåˆ™ä¸å¯åŠ¨æ–°æµè§ˆå™¨
        if self.use_external_page:
            self.page = self.external_page
            print("âœ“ ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„æµè§ˆå™¨é¡µé¢ï¼ˆé¡µé¢é©±åŠ¨æ¨¡å¼ï¼‰")
            return

        if not PLAYWRIGHT_AVAILABLE:
            raise Exception("Playwrightæœªå®‰è£…ï¼Œæ— æ³•å¯åŠ¨æµè§ˆå™¨")

        print("\nğŸŒ å¯åŠ¨æµè§ˆå™¨")
        self.playwright = sync_playwright().start()

        max_retries = 2
        for attempt in range(max_retries):
            try:
                self.browser = self.playwright.chromium.launch(headless=self.headless, proxy=None)
                break
            except Exception as e:
                if "Executable doesn't exist" in str(e) and attempt == 0:
                    if self._install_browser():
                        continue
                    else:
                        raise Exception("æµè§ˆå™¨å®‰è£…å¤±è´¥")
                raise e

        use_saved_state = os.path.exists(self.state_file)

        if use_saved_state:
            print(f"âœ“ æ£€æµ‹åˆ°çŠ¶æ€æ–‡ä»¶: {self.state_file}")
            try:
                self.context = self.browser.new_context(
                    storage_state=self.state_file,
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    proxy=None, bypass_csp=True, ignore_https_errors=True
                )
                self.page = self.context.new_page()
                if self._check_login_status():
                    print(f"âœ“ æµè§ˆå™¨å·²å¯åŠ¨ï¼ˆä½¿ç”¨ä¿å­˜çš„çŠ¶æ€ï¼‰")
                    return
                else:
                    self.context.close()
                    use_saved_state = False
            except Exception as e:
                print(f"âš ï¸ çŠ¶æ€æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                if self.context:
                    self.context.close()
                use_saved_state = False

        if not use_saved_state:
            print("æ­£åœ¨ä½¿ç”¨Cookieç™»å½•...")
            playwright_cookies = self._convert_cookies_to_playwright_format(self.cookies)
            self.context = self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                proxy=None, bypass_csp=True, ignore_https_errors=True
            )
            self.context.add_cookies(playwright_cookies)
            self.page = self.context.new_page()

            if not self._check_login_status():
                # çŠ¶æ€æ–‡ä»¶ç™»å½•å¤±è´¥ä¸”API cookieç™»å½•ä¹Ÿå¤±è´¥ï¼Œä¸ŠæŠ¥è´¦æˆ·å¤±æ•ˆçŠ¶æ€
                report_auth_invalid(self.account_name)
                raise Exception("Cookieç™»å½•å¤±è´¥")

            self.context.storage_state(path=self.state_file)
            print(f"âœ“ æµè§ˆå™¨å·²å¯åŠ¨ï¼ˆCookieç™»å½•ï¼‰")

    def stop_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        # å¦‚æœä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ pageï¼Œåˆ™ä¸å…³é—­æµè§ˆå™¨ï¼ˆç”±å¤–éƒ¨ç®¡ç†ï¼‰
        if self.use_external_page:
            print("âœ“ å¤–éƒ¨æµè§ˆå™¨é¡µé¢ä¿æŒæ‰“å¼€ï¼ˆç”±å¤–éƒ¨ç®¡ç†ï¼‰")
            return

        if self.context:
            self.context.close()
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        print("âœ“ æµè§ˆå™¨å·²å…³é—­")

    def _get_mtgsig(self) -> str:
        """è·å–mtgsig"""
        if self.mtgsig_from_api:
            return self.mtgsig_from_api
        timestamp = int(time.time() * 1000)
        webdfpid = self.cookies.get('WEBDFPID', '')
        a3 = webdfpid.split('-')[0] if webdfpid and '-' in webdfpid else ''
        mtgsig = {"a1": "1.2", "a2": timestamp, "a3": a3, "a5": "", "a6": "", "a8": "", "a9": "4.1.1,7,139", "a10": "9a", "x0": 4, "d1": ""}
        return json.dumps(mtgsig)

    def _get_headers(self) -> Dict[str, str]:
        """è·å–é€šç”¨è¯·æ±‚å¤´"""
        return {
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Connection': 'keep-alive',
            'Content-Type': 'application/x-www-form-urlencoded',
            'Origin': 'https://h5.dianping.com',
            'Referer': 'https://h5.dianping.com/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def _calculate_flow_date_range(self) -> str:
        """è®¡ç®—å®¢æµæ•°æ®çš„æ—¥æœŸèŒƒå›´"""
        now = datetime.now()
        if now.hour < 7:
            end_date = now - timedelta(days=2)
        else:
            end_date = now - timedelta(days=1)
        start_date = end_date - timedelta(days=6)
        return f"{start_date.strftime('%Y-%m-%d')},{end_date.strftime('%Y-%m-%d')}"

    def _get_yesterday_date(self) -> str:
        """è·å–æ˜¨å¤©æ—¥æœŸ"""
        return (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

    def _parse_rank_value(self, value) -> int:
        """è§£ææ’åå€¼"""
        if pd.isna(value) or value == '' or value is None:
            return 0
        value_str = str(value).strip().replace('+', '')
        try:
            return int(float(value_str))
        except:
            return 0

    def get_force_offline_data(self, target_date: str) -> Dict[str, int]:
        """è·å–å¼ºåˆ¶ä¸‹çº¿æ•°æ®ï¼ˆä½¿ç”¨æµè§ˆå™¨ç¯å¢ƒï¼‰"""
        print("\nğŸ“‹ è·å–å¼ºåˆ¶ä¸‹çº¿æ•°æ®ï¼ˆæµè§ˆå™¨æ¨¡å¼ï¼‰")
        print(f"   ç›®æ ‡æ—¥æœŸ: {target_date}")
        force_offline_count = {}

        try:
            self.page.goto(
                "https://e.dianping.com/app/vg-pc-platform-merchant-selfhelp/newNoticeCenter.html",
                wait_until='networkidle', timeout=30000
            )
            time.sleep(3)

            api_url = "https://e.dianping.com/gateway/msg/MessageDzService/queryPcMessageList"
            target_date_obj = datetime.strptime(target_date, '%Y-%m-%d').date()

            script = f"""
            async () => {{
                try {{
                    const response = await fetch('{api_url}?yodaReady=h5&csecplatform=4&csecversion=4.1.1', {{
                        method: 'POST', credentials: 'include',
                        headers: {{'Content-Type': 'application/json'}},
                        body: JSON.stringify({{"messageCategoryCode": 0, "status": null, "subCategoryIdList": null, "important": 1, "pageNo": 1, "pageSize": 100}})
                    }});
                    return {{success: true, data: await response.json()}};
                }} catch(e) {{
                    return {{success: false, error: e.message}};
                }}
            }}
            """

            result = self.page.evaluate(script)
            if not result.get('success'):
                print(f"âŒ APIè°ƒç”¨å¤±è´¥: {result.get('error')}")
                return force_offline_count

            api_result = result.get('data', {})
            if api_result.get('status') != 0:
                print(f"âŒ APIè¿”å›é”™è¯¯")
                return force_offline_count

            message_list = api_result.get('messageList', [])
            print(f"   è·å–åˆ° {len(message_list)} æ¡æ¶ˆæ¯")

            for msg in message_list:
                title = msg.get('title', '')
                create_time = msg.get('createTime', 0)
                if 'å¼ºåˆ¶ä¸‹çº¿' not in title:
                    continue
                if create_time:
                    msg_date = datetime.fromtimestamp(create_time / 1000).date()
                    if msg_date != target_date_obj:
                        continue
                    shop_id = msg.get('mtShopId') or self.shop_id
                    if shop_id:
                        shop_id_str = str(shop_id)
                        force_offline_count[shop_id_str] = force_offline_count.get(shop_id_str, 0) + 1
                        print(f"   ğŸ“Œ å‘ç°å¼ºåˆ¶ä¸‹çº¿: é—¨åº—{shop_id_str}")

            print(f"âœ… å¼ºåˆ¶ä¸‹çº¿ç»Ÿè®¡å®Œæˆ: {force_offline_count}")
            return force_offline_count
        except Exception as e:
            print(f"âŒ è·å–å¼ºåˆ¶ä¸‹çº¿æ•°æ®å¤±è´¥: {e}")
            return force_offline_count

    def get_flow_data(self) -> Dict[str, int]:
        """è·å–å®¢æµæ•°æ®ï¼ˆæ‰“å¡æ•°ï¼‰"""
        print("\nğŸ“‹ è·å–å®¢æµæ•°æ®ï¼ˆæ‰“å¡æ•°ï¼‰")
        url = "https://e.dianping.com/gateway/adviser/data"
        date_range = self._calculate_flow_date_range()
        print(f"   æ—¥æœŸèŒƒå›´: {date_range}")

        params = {'componentId': 'flowDataSummaryDownloadPCAsync', 'pageType': 'flowAnalysis',
                  'yodaReady': 'h5', 'csecplatform': '4', 'csecversion': '4.1.1', 'mtgsig': self._get_mtgsig()}
        post_data = {'source': '1', 'device': 'pc', 'pageType': 'flowAnalysis', 'shopIds': '0', 'platform': '0', 'date': date_range}
        checkin_data = {}

        try:
            session = self._get_session()
            response = session.post(url, params=params, data=post_data, headers=self._get_headers(), cookies=self.cookies, timeout=60)
            response.raise_for_status()
            result = response.json()

            if result.get('code') != 200:
                print(f"âŒ APIè¿”å›é”™è¯¯")
                return checkin_data

            data_list = result.get('data', [])
            file_url = None
            for item in data_list:
                body = item.get('body', {})
                if body.get('fileUrl'):
                    file_url = body.get('fileUrl')
                    break

            if not file_url:
                print("âŒ æœªè·å–åˆ°æ–‡ä»¶URL")
                return checkin_data

            random_delay()  # åçˆ¬è™«ç­‰å¾…
            print(f"   ğŸ“¥ ä¸‹è½½æ–‡ä»¶...")
            file_response = session.get(file_url, timeout=60)
            df = pd.read_excel(BytesIO(file_response.content))
            print(f"   ğŸ“Š è¯»å–åˆ° {len(df)} è¡Œæ•°æ®")

            date_col = df.columns[0]
            df[date_col] = pd.to_datetime(df[date_col])
            latest_date = df[date_col].max()
            latest_df = df[df[date_col] == latest_date]

            shop_id_col = df.columns[3]
            checkin_col = df.columns[36]  # AMåˆ— - ç¬¬37åˆ— - æ‰“å¡æ•°

            for _, row in latest_df.iterrows():
                shop_id = str(int(row[shop_id_col])) if pd.notna(row[shop_id_col]) else None
                checkin_count = int(row[checkin_col]) if pd.notna(row[checkin_col]) else 0
                if shop_id:
                    checkin_data[shop_id] = checkin_count

            print(f"âœ… å®¢æµæ•°æ®è·å–å®Œæˆ: {len(checkin_data)} ä¸ªé—¨åº—")
            return checkin_data
        except Exception as e:
            print(f"âŒ è·å–å®¢æµæ•°æ®å¤±è´¥: {e}")
            return checkin_data

    def get_rival_rank_data(self) -> Dict[str, Dict[str, int]]:
        """è·å–åŒè¡Œæ’åæ•°æ®"""
        print("\nğŸ“‹ è·å–åŒè¡Œæ’åæ•°æ®")
        rank_data = {}

        if not self.shop_region_info:
            print("âš ï¸ æ²¡æœ‰é—¨åº—å•†åœˆä¿¡æ¯ï¼Œè·³è¿‡æ’åæ•°æ®è·å–")
            for shop in self.shop_list:
                rank_data[shop['shop_id']] = {'order_user_rank': 0, 'verify_amount_rank': 0}
            return rank_data

        for shop in self.shop_list:
            shop_id = shop['shop_id']
            shop_name = shop['shop_name']
            shop_info = self.shop_region_info.get(shop_id, {})
            regions = shop_info.get('regions', {})
            business = regions.get('business', {})
            region_id = business.get('regionId')

            if not region_id:
                rank_data[shop_id] = {'order_user_rank': 0, 'verify_amount_rank': 0}
                continue

            print(f"   ğŸª è·å–é—¨åº— {shop_name}({shop_id}) çš„æ’åæ•°æ®...")
            shop_rank = self._get_rival_rank_by_shop(shop_id, region_id)
            rank_data[shop_id] = shop_rank
            print(f"      ä¸‹å•æ’å: {shop_rank['order_user_rank']}, æ ¸é”€æ’å: {shop_rank['verify_amount_rank']}")
            random_delay()  # åçˆ¬è™«ç­‰å¾…

        print(f"âœ… åŒè¡Œæ’åæ•°æ®è·å–å®Œæˆ: {len(rank_data)} ä¸ªé—¨åº—")
        return rank_data

    def _get_rival_rank_by_shop(self, shop_id: str, region_id: int) -> Dict[str, int]:
        """è·å–æŒ‡å®šé—¨åº—çš„åŒè¡Œæ’åæ•°æ®"""
        url = "https://e.dianping.com/gateway/adviser/data"
        params = {
            'device': 'pc', 'source': '1', 'pageType': 'rivalAnalysisV2', 'sign': '', 'dateType': '1',
            'platform': '0', 'shopIds': shop_id, 'regionId': str(region_id), 'regionType': 'å•†åœˆ',
            'componentId': 'shopRankListDownload', 'yodaReady': 'h5', 'csecplatform': '4', 'csecversion': '4.1.1', 'mtgsig': self._get_mtgsig()
        }
        headers = {
            'Accept': 'application/json, text/plain, */*', 'Referer': 'https://e.dianping.com/codejoy/2703/home/index.html',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        default_result = {'order_user_rank': 0, 'verify_amount_rank': 0}

        try:
            session = self._get_session()
            response = session.get(url, params=params, headers=headers, cookies=self.cookies, timeout=60)
            response.raise_for_status()
            result = response.json()

            if result.get('code') != 200:
                return default_result

            data_list = result.get('data', [])
            file_url = None
            for item in data_list:
                body = item.get('body', {})
                if body.get('fileUrl'):
                    file_url = body.get('fileUrl')
                    break

            if not file_url:
                return default_result

            file_response = session.get(file_url, timeout=60)
            df = pd.read_excel(BytesIO(file_response.content))

            if len(df) == 0:
                return default_result

            shop_id_col = df.columns[4]
            order_rank_col = df.columns[10]
            verify_rank_col = df.columns[14]

            for _, row in df.iterrows():
                row_shop_id = str(int(row[shop_id_col])) if pd.notna(row[shop_id_col]) else None
                if row_shop_id == shop_id:
                    return {
                        'order_user_rank': self._parse_rank_value(row[order_rank_col]),
                        'verify_amount_rank': self._parse_rank_value(row[verify_rank_col])
                    }
            return default_result
        except Exception as e:
            print(f"      âŒ è·å–æ’åæ•°æ®å¤±è´¥: {e}")
            return default_result

    def get_trade_data(self) -> Dict[str, int]:
        """è·å–å•†å“äº¤æ˜“æ•°æ®ï¼ˆå¹¿å‘Šå•ï¼‰"""
        print("\nğŸ“‹ è·å–å•†å“äº¤æ˜“æ•°æ®ï¼ˆå¹¿å‘Šå•ï¼‰")
        ad_data = {}
        for shop in self.shop_list:
            ad_data[shop['shop_id']] = 0

        if not self.product_mapping:
            print("âš ï¸ æ²¡æœ‰å›¢è´­IDæ˜ å°„ï¼Œè·³è¿‡å¹¿å‘Šå•æ•°æ®è·å–")
            return ad_data

        shop_to_brands = {item['shop_id']: item['brands_id'] for item in self.product_mapping}
        url = "https://e.dianping.com/gateway/adviser/data"
        yesterday = self._get_yesterday_date()
        timestamp = int(time.time() * 1000)

        params = {'componentId': 'shopTradeProductRankDownload', 'pageType': 'v5Trade',
                  'yodaReady': 'h5', 'csecplatform': '4', 'csecversion': '4.1.1', 'mtgsig': self._get_mtgsig()}
        post_data = {
            'optionType': 'v5Trade', 'typeIds': '7', 'sortTypeId': '7',
            'prdIds': '1,2,3,4,5,6,11,12,13,14,15,16,17,18,19,20', 'source': '1', 'device': 'pc',
            'date': f'{yesterday},{yesterday}', 'platform': '0', 'pageType': 'v5Trade',
            'shopIds': '', 'excludeShopIds': '', 'cityId': '', 'spuId': '', 'pageNum': '', 'pageSize': '',
            'sign': '', 'fromPage': '', 'storeKey': self.shop_id, 'timeStamp': str(timestamp), 'downloadAllPrdIds': 'true'
        }

        try:
            session = self._get_session()
            response = session.post(url, params=params, data=post_data, headers=self._get_headers(), cookies=self.cookies, timeout=60)
            response.raise_for_status()
            result = response.json()

            if result.get('code') != 200:
                return ad_data

            data_list = result.get('data', [])
            file_url = None
            for item in data_list:
                body = item.get('body', {})
                if body.get('fileUrl'):
                    file_url = body.get('fileUrl')
                    break

            if not file_url:
                return ad_data

            random_delay()  # åçˆ¬è™«ç­‰å¾…
            print(f"   ğŸ“¥ ä¸‹è½½æ–‡ä»¶...")
            file_response = session.get(file_url, timeout=60)
            df = pd.read_excel(BytesIO(file_response.content))
            print(f"   ğŸ“Š è¯»å–åˆ° {len(df)} è¡Œæ•°æ®")

            product_id_col = df.columns[2]
            shop_id_col = df.columns[6]
            order_count_col = df.columns[8]

            for _, row in df.iterrows():
                product_id = str(int(row[product_id_col])) if pd.notna(row[product_id_col]) else None
                row_shop_id = str(int(row[shop_id_col])) if pd.notna(row[shop_id_col]) else None
                order_count = int(row[order_count_col]) if pd.notna(row[order_count_col]) else 0

                if row_shop_id and row_shop_id in shop_to_brands:
                    if product_id == shop_to_brands[row_shop_id]:
                        ad_data[row_shop_id] = order_count
                        print(f"   ğŸ“Œ æ‰¾åˆ°: é—¨åº—ID={row_shop_id}, ä¸‹å•äººæ•°={order_count}")

            print(f"âœ… å•†å“äº¤æ˜“æ•°æ®è·å–å®Œæˆ")
            return ad_data
        except Exception as e:
            print(f"âŒ è·å–å•†å“äº¤æ˜“æ•°æ®å¤±è´¥: {e}")
            return ad_data

    def get_finance_balance(self) -> float:
        """
        è·å–è´¢åŠ¡ä½™é¢ï¼ˆç»¼åˆæ¨å¹¿ä½™é¢ï¼‰

        Returns:
            ä½™é¢é‡‘é¢ï¼Œå¤±è´¥æ—¶è¿”å›0
        """
        print("\nğŸ’° è·å–è´¢åŠ¡ä½™é¢æ•°æ®")

        url = "https://e.dianping.com/adpaccount/finance/account/r/getHomeFinancialDetail"

        headers = {
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Referer': 'https://e.dianping.com/app/peon-promo-finance/html/flow-home.html',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
            'X-Requested-With': 'XMLHttpRequest',
            'sec-ch-ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
        }

        try:
            session = self._get_session()
            response = session.get(
                url,
                headers=headers,
                cookies=self.cookies,
                timeout=30
            )

            response.raise_for_status()
            result = response.json()

            if result.get('code') != 0:
                print(f"âŒ APIè¿”å›é”™è¯¯: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
                return 0.0

            data_list = result.get('data', [])
            if not data_list:
                print("âŒ æœªè·å–åˆ°è´¢åŠ¡æ•°æ®")
                return 0.0

            # æŸ¥æ‰¾"ç»¼åˆæ¨å¹¿"çš„ä½™é¢
            for item in data_list:
                product_name = item.get('productName', '')
                if product_name == 'ç»¼åˆæ¨å¹¿':
                    balance = item.get('totalBalance', 0)
                    print(f"âœ… è´¢åŠ¡ä½™é¢è·å–æˆåŠŸ")
                    print(f"   ç»¼åˆæ¨å¹¿ä½™é¢: Â¥{balance}")
                    return float(balance)

            # å¦‚æœæ²¡æ‰¾åˆ°"ç»¼åˆæ¨å¹¿"ï¼Œè¿”å›ç¬¬ä¸€ä¸ªäº§å“çš„ä½™é¢
            if data_list:
                first_item = data_list[0]
                balance = first_item.get('totalBalance', 0)
                product_name = first_item.get('productName', 'æœªçŸ¥')
                print(f"âš ï¸ æœªæ‰¾åˆ°'ç»¼åˆæ¨å¹¿'ï¼Œä½¿ç”¨'{product_name}'çš„ä½™é¢")
                print(f"   ä½™é¢: Â¥{balance}")
                return float(balance)

            return 0.0

        except Exception as e:
            print(f"âŒ è·å–è´¢åŠ¡ä½™é¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0.0

    def collect_and_upload(self, target_date: str, upload_api_url: str) -> bool:
        """æ”¶é›†æ‰€æœ‰æ•°æ®å¹¶ä¸Šä¼ """
        print("\nğŸš€ å¼€å§‹æ”¶é›†å’Œä¸Šä¼ æ•°æ®")
        print(f"   ç›®æ ‡æ—¥æœŸ: {target_date}")
        print(f"   é—¨åº—æ•°é‡: {len(self.shop_list)}")

        try:
            self.start_browser()
            force_offline_data = self.get_force_offline_data(target_date)
            random_delay()  # åçˆ¬è™«ç­‰å¾…
            finance_balance = self.get_finance_balance()
            random_delay()  # åçˆ¬è™«ç­‰å¾…
            checkin_data = self.get_flow_data()
            random_delay()  # åçˆ¬è™«ç­‰å¾…
            rank_data = self.get_rival_rank_data()
            random_delay()  # åçˆ¬è™«ç­‰å¾…
            ad_data = self.get_trade_data()
        finally:
            self.stop_browser()

        # æ›´æ–°å…±äº«ç­¾å
        global SHARED_SIGNATURE
        SHARED_SIGNATURE['mtgsig'] = self.mtgsig_from_api
        SHARED_SIGNATURE['cookies'] = self.cookies
        SHARED_SIGNATURE['updated_at'] = datetime.now()
        SHARED_SIGNATURE['shop_list'] = self.shop_list
        print(f"âœ… å·²æ›´æ–°å…±äº«ç­¾åï¼Œä¾›åç»­ä»»åŠ¡ä½¿ç”¨")

        # æ•´åˆæ•°æ®
        print("\nğŸ“Š æ•´åˆæ•°æ®")
        upload_data_list = []

        for shop in self.shop_list:
            shop_id = shop['shop_id']
            shop_name = shop['shop_name']
            data = {
                "store_name": shop_name,
                "store_id": int(shop_id),
                "checkin_count": checkin_data.get(shop_id, 0),
                "order_user_rank": rank_data.get(shop_id, {}).get('order_user_rank', 0),
                "verify_amount_rank": rank_data.get(shop_id, {}).get('verify_amount_rank', 0),
                "ad_order_count": ad_data.get(shop_id, 0),
                "ad_balance": finance_balance,
                "is_force_offline": force_offline_data.get(shop_id, 0),
                "date": target_date
            }
            upload_data_list.append(data)
            print(f"   ğŸ“Œ é—¨åº—: {shop_name} ({shop_id}) - æ‰“å¡:{data['checkin_count']}, ä¸‹å•æ’å:{data['order_user_rank']}, å¹¿å‘Šå•:{data['ad_order_count']}, å¹¿å‘Šä½™é¢:Â¥{finance_balance}, å¼ºåˆ¶ä¸‹çº¿:{data['is_force_offline']}")

        # ä¸Šä¼ æ•°æ®
        print(f"\nğŸ“¤ ä¸Šä¼ æ•°æ®åˆ°API: {upload_api_url}")
        session = self._get_session()
        success_count = 0
        fail_count = 0

        for idx, data in enumerate(upload_data_list, 1):
            try:
                response = session.post(upload_api_url, json=data, headers={'Content-Type': 'application/json'}, timeout=30)
                if response.status_code in [200, 201]:
                    success_count += 1
                    print(f"   [{idx}/{len(upload_data_list)}] âœ… æˆåŠŸ - {data['store_name']}")
                else:
                    fail_count += 1
                    print(f"   [{idx}/{len(upload_data_list)}] âŒ å¤±è´¥ - {data['store_name']}")
            except Exception as e:
                fail_count += 1
                print(f"   [{idx}/{len(upload_data_list)}] âŒ å¤±è´¥ - {data['store_name']}: {e}")

        print(f"\nğŸ“Š ä¸Šä¼ å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")
        return fail_count == 0


# ============================================================================
# run_store_stats ä»»åŠ¡å‡½æ•°
# ============================================================================
def run_store_stats(account_name: str, start_date: str, end_date: str, external_page=None) -> Dict[str, Any]:
    """æ‰§è¡Œstore_statsä»»åŠ¡ - é—¨åº—ç»Ÿè®¡æ•°æ®é‡‡é›†

    Args:
        account_name: è´¦æˆ·åç§°
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        external_page: å¤–éƒ¨ä¼ å…¥çš„ Playwright page å¯¹è±¡ï¼ˆç”¨äºé¡µé¢é©±åŠ¨æ¨¡å¼ï¼‰
    """
    table_name = "store_stats"
    print(f"\n{'=' * 60}")
    if external_page:
        print(f"ğŸª {table_name} (é—¨åº—ç»Ÿè®¡ - é¡µé¢é©±åŠ¨æ¨¡å¼)")
    else:
        print(f"ğŸª {table_name} (é—¨åº—ç»Ÿè®¡ - Playwrightæµè§ˆå™¨æ¨¡å¼)")
    print(f"{'=' * 60}")

    result = {"task_name": table_name, "success": False, "record_count": 0, "error_message": "æ— "}

    # æ£€æŸ¥Playwrightæ˜¯å¦å¯ç”¨ï¼ˆä»…åœ¨éé¡µé¢é©±åŠ¨æ¨¡å¼ä¸‹æ£€æŸ¥ï¼‰
    if not external_page and not PLAYWRIGHT_AVAILABLE:
        error_msg = "Playwrightæœªå®‰è£…ï¼Œstore_statsä»»åŠ¡è·³è¿‡"
        print(f"âŒ {error_msg}")
        result["error_message"] = error_msg
        log_failure(account_name, 0, table_name, start_date, end_date, error_msg)
        return result

    # è®¡ç®—ç›®æ ‡æ—¥æœŸï¼ˆä¼˜å…ˆä½¿ç”¨TARGET_DATEï¼Œå¦åˆ™ä½¿ç”¨END_DATEï¼‰
    if TARGET_DATE:
        target_date = TARGET_DATE
    else:
        target_date = END_DATE

    print(f"   ç›®æ ‡æ—¥æœŸ: {target_date}")
    if external_page:
        print(f"   æµè§ˆå™¨æ¨¡å¼: é¡µé¢é©±åŠ¨æ¨¡å¼ï¼ˆå¤ç”¨å¤–éƒ¨æµè§ˆå™¨ï¼‰")
    else:
        print(f"   æµè§ˆå™¨æ¨¡å¼: {'æ— å¤´æ¨¡å¼' if HEADLESS else 'å¯è§†æ¨¡å¼'}")

    try:
        disable_proxy()

        # åˆ›å»ºé‡‡é›†å™¨
        collector = DianpingStoreStats(
            account_name,
            PLATFORM_ACCOUNTS_API_URL,
            headless=HEADLESS,
            disable_proxy=True,
            external_page=external_page
        )

        # æ‰§è¡Œé‡‡é›†å’Œä¸Šä¼ 
        success = collector.collect_and_upload(
            target_date=target_date,
            upload_api_url=UPLOAD_APIS[table_name]
        )

        if success:
            result["success"] = True
            result["record_count"] = len(collector.shop_list)
            for shop in collector.shop_list:
                log_success(account_name, int(shop['shop_id']), table_name, target_date, target_date, 1)
        else:
            result["error_message"] = "éƒ¨åˆ†æ•°æ®ä¸Šä¼ å¤±è´¥"
            log_failure(account_name, 0, table_name, target_date, target_date, result["error_message"])

    except Exception as e:
        error_msg = str(e)
        result["error_message"] = error_msg
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        # ä¸ŠæŠ¥åˆ° /api/log
        log_failure(account_name, 0, table_name, start_date, end_date, error_msg)
        # å¦‚æœæ˜¯ç™»å½•å¤±è´¥ï¼ŒåŒæ—¶ä¸ŠæŠ¥åˆ° /api/account_task/update_batch
        if "ç™»å½•å¤±è´¥" in error_msg or "Cookieç™»å½•å¤±è´¥" in error_msg:
            upload_task_status_single(account_name, start_date, end_date, {
                'task_name': table_name,
                'success': False,
                'record_count': 0,
                'error_message': error_msg
            })

    return result


# ============================================================================
# é¡µé¢é©±åŠ¨ä»»åŠ¡æ‰§è¡Œç±» - å…ˆè·³è½¬é¡µé¢å†æ‰§è¡Œå¯¹åº”ä»»åŠ¡
# ============================================================================
class PageDrivenTaskExecutor:
    """é¡µé¢é©±åŠ¨çš„ä»»åŠ¡æ‰§è¡Œå™¨

    å·¥ä½œæµç¨‹:
    1. å¯åŠ¨ Playwright æµè§ˆå™¨
    2. æŒ‰é¡ºåºè·³è½¬åˆ°å„ä¸ªé¡µé¢
    3. åœ¨æ¯ä¸ªé¡µé¢ä¸Šæ‰§è¡Œå¯¹åº”çš„ä»»åŠ¡
    4. å…³é—­æµè§ˆå™¨

    æ‰§è¡Œé¡ºåº:
    - æŠ¥è¡¨é¡µé¢: kewen_daily_report, promotion_daily_report
    - å®¢æµåˆ†æé¡µé¢: store_stats
    - è¯„ä»·é¡µé¢(æœ€å): review_detail_dianping, review_detail_meituan,
                      review_summary_dianping, review_summary_meituan
    """

    PAGE_NAME_MAP = {
        "report": "æŠ¥è¡¨é¡µé¢",
        "flow_analysis": "å®¢æµåˆ†æé¡µé¢",
        "review": "è¯„ä»·é¡µé¢",
    }

    def __init__(self, account_name: str, headless: bool = True):
        """åˆå§‹åŒ–

        Args:
            account_name: è´¦æˆ·åç§°
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
        """
        self.account_name = account_name
        self.headless = headless
        self.state_file = os.path.join(STATE_DIR, f'dianping_state_{account_name}.json')

        # Playwrightç›¸å…³
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None

        # ä»APIè·å–çš„æ•°æ®
        self.cookies = {}
        self.mtgsig = None
        self.shop_info = {}
        self.templates_id = None

        # æ‰§è¡Œç»“æœ
        self.results = []

    def _disable_proxy(self):
        """ç¦ç”¨ç³»ç»Ÿä»£ç†"""
        proxy_vars = [
            'HTTP_PROXY', 'HTTPS_PROXY', 'FTP_PROXY', 'SOCKS_PROXY',
            'http_proxy', 'https_proxy', 'ftp_proxy', 'socks_proxy',
            'ALL_PROXY', 'all_proxy', 'NO_PROXY', 'no_proxy'
        ]
        for var in proxy_vars:
            os.environ.pop(var, None)
        os.environ['NO_PROXY'] = '*'
        os.environ['no_proxy'] = '*'
        print("âœ… å·²ç¦ç”¨ç³»ç»Ÿä»£ç†")

    def _load_account_info(self):
        """ä»APIåŠ è½½è´¦æˆ·ä¿¡æ¯"""
        print(f"\nğŸ” æ­£åœ¨ä»APIè·å–è´¦æˆ· [{self.account_name}] çš„ä¿¡æ¯...")
        api_data = load_cookies_from_api(self.account_name)
        self.cookies = api_data['cookies']
        self.mtgsig = api_data['mtgsig']
        self.shop_info = api_data['shop_info']
        self.templates_id = api_data['templates_id']
        print(f"âœ… è´¦æˆ·ä¿¡æ¯åŠ è½½å®Œæˆ")

    def _convert_cookies_to_playwright_format(self) -> list:
        """å°†cookieå­—å…¸è½¬æ¢ä¸ºPlaywrightæ ¼å¼"""
        playwright_cookies = []
        for name, value in self.cookies.items():
            cookie = {
                'name': name,
                'value': str(value),
                'domain': '.dianping.com',
                'path': '/'
            }
            playwright_cookies.append(cookie)
        return playwright_cookies

    def _check_login_status(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¤„äºç™»å½•çŠ¶æ€"""
        try:
            self.page.goto(
                "https://e.dianping.com/app/vg-pc-platform-merchant-selfhelp/newNoticeCenter.html",
                wait_until='networkidle',
                timeout=15000
            )
            time.sleep(2)
            current_url = self.page.url
            if 'login' in current_url.lower():
                return False
            has_content = self.page.evaluate("() => document.body.textContent.length > 100")
            return has_content
        except Exception as e:
            print(f"âœ— ç™»å½•æ£€æµ‹å¤±è´¥: {e}")
            return False

    def _install_browser(self):
        """è‡ªåŠ¨å®‰è£…Playwrightæµè§ˆå™¨"""
        print("\nâš ï¸ æ£€æµ‹åˆ°Chromiumæµè§ˆå™¨æœªå®‰è£…ï¼Œæ­£åœ¨è‡ªåŠ¨ä¸‹è½½...")
        try:
            process = subprocess.Popen(
                [sys.executable, '-m', 'playwright', 'install', 'chromium'],
                stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1
            )
            for line in process.stdout:
                print(line.strip())
            process.wait()
            return process.returncode == 0
        except Exception as e:
            print(f"å®‰è£…å¤±è´¥: {e}")
            return False

    def start_browser(self):
        """å¯åŠ¨æµè§ˆå™¨å¹¶ç™»å½•"""
        if not PLAYWRIGHT_AVAILABLE:
            raise Exception("Playwrightæœªå®‰è£…ï¼Œæ— æ³•å¯åŠ¨æµè§ˆå™¨")

        print("\nğŸŒ å¯åŠ¨æµè§ˆå™¨")
        self.playwright = sync_playwright().start()

        max_retries = 2
        for attempt in range(max_retries):
            try:
                self.browser = self.playwright.chromium.launch(
                    headless=self.headless,
                    proxy=None
                )
                break
            except Exception as e:
                if "Executable doesn't exist" in str(e) and attempt == 0:
                    if self._install_browser():
                        continue
                    else:
                        raise Exception("æµè§ˆå™¨å®‰è£…å¤±è´¥")
                raise e

        use_saved_state = os.path.exists(self.state_file)

        if use_saved_state:
            print(f"âœ“ æ£€æµ‹åˆ°çŠ¶æ€æ–‡ä»¶: {self.state_file}")
            try:
                self.context = self.browser.new_context(
                    storage_state=self.state_file,
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    proxy=None,
                    bypass_csp=True,
                    ignore_https_errors=True
                )
                self.page = self.context.new_page()
                if self._check_login_status():
                    print(f"âœ“ æµè§ˆå™¨å·²å¯åŠ¨ï¼ˆä½¿ç”¨ä¿å­˜çš„çŠ¶æ€ï¼‰")
                    return
                else:
                    self.context.close()
                    use_saved_state = False
            except Exception as e:
                print(f"âš ï¸ çŠ¶æ€æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                if self.context:
                    self.context.close()
                use_saved_state = False

        if not use_saved_state:
            print("æ­£åœ¨ä½¿ç”¨Cookieç™»å½•...")
            playwright_cookies = self._convert_cookies_to_playwright_format()
            self.context = self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                proxy=None,
                bypass_csp=True,
                ignore_https_errors=True
            )
            self.context.add_cookies(playwright_cookies)
            self.page = self.context.new_page()

            if not self._check_login_status():
                report_auth_invalid(self.account_name)
                raise Exception("Cookieç™»å½•å¤±è´¥")

            self.context.storage_state(path=self.state_file)
            print(f"âœ“ æµè§ˆå™¨å·²å¯åŠ¨ï¼ˆCookieç™»å½•ï¼‰")

    def stop_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.context:
            self.context.close()
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        print("âœ“ æµè§ˆå™¨å·²å…³é—­")

    def navigate_to_page(self, page_key: str):
        """è·³è½¬åˆ°æŒ‡å®šé¡µé¢

        Args:
            page_key: é¡µé¢é”®å (report, flow_analysis, review)
        """
        page_url = PAGE_URLS.get(page_key)
        page_name = self.PAGE_NAME_MAP.get(page_key, page_key)

        if not page_url:
            print(f"âš ï¸ æœªæ‰¾åˆ°é¡µé¢URL: {page_key}")
            return False

        print(f"\n{'=' * 60}")
        print(f"ğŸ”— æ­£åœ¨è·³è½¬åˆ° {page_name}...")
        print(f"   URL: {page_url[:80]}...")
        print(f"{'=' * 60}")

        try:
            self.page.goto(page_url, wait_until='networkidle', timeout=30000)
            time.sleep(3)  # ç­‰å¾…é¡µé¢ç¨³å®š
            print(f"âœ… å·²è·³è½¬åˆ° {page_name}")
            return True
        except Exception as e:
            print(f"âŒ è·³è½¬å¤±è´¥: {e}")
            return False

    def execute_page_tasks(self, page_key: str, start_date: str, end_date: str) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒæŒ‡å®šé¡µé¢çš„æ‰€æœ‰ä»»åŠ¡

        Args:
            page_key: é¡µé¢é”®å
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            ä»»åŠ¡æ‰§è¡Œç»“æœåˆ—è¡¨
        """
        tasks = PAGE_TASKS.get(page_key, [])
        page_name = self.PAGE_NAME_MAP.get(page_key, page_key)
        results = []

        print(f"\nğŸ“‹ {page_name} éœ€è¦æ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡: {', '.join(tasks)}")

        for task_name in tasks:
            print(f"\n{'â”€' * 50}")
            print(f"â–¶ å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_name}")
            print(f"{'â”€' * 50}")

            task_func = TASK_MAP.get(task_name)
            if task_func:
                # å¯¹äº store_stats ä»»åŠ¡ï¼Œä¼ é€’å½“å‰ page å¯¹è±¡ï¼ˆé¡µé¢é©±åŠ¨æ¨¡å¼ï¼‰
                if task_name == 'store_stats':
                    result = task_func(self.account_name, start_date, end_date, external_page=self.page)
                else:
                    result = task_func(self.account_name, start_date, end_date)
                results.append(result)

                if result.get('success'):
                    print(f"âœ… ä»»åŠ¡ {task_name} æ‰§è¡ŒæˆåŠŸ")
                else:
                    print(f"âŒ ä»»åŠ¡ {task_name} æ‰§è¡Œå¤±è´¥: {result.get('error_message')}")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°ä»»åŠ¡å‡½æ•°: {task_name}")
                results.append({
                    "task_name": task_name,
                    "success": False,
                    "record_count": 0,
                    "error_message": f"æœªæ‰¾åˆ°ä»»åŠ¡å‡½æ•°"
                })

            # ä»»åŠ¡é—´éšæœºå»¶è¿Ÿ
            random_delay(2, 4)

        return results

    def run_all_tasks(self, start_date: str, end_date: str) -> List[Dict[str, Any]]:
        """æŒ‰é¡µé¢é¡ºåºæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡

        æ‰§è¡Œé¡ºåº:
        1. å®¢æµåˆ†æé¡µé¢: store_stats (å…ˆæ‰§è¡Œï¼Œæ›´æ–°ç­¾å)
        2. æŠ¥è¡¨é¡µé¢: kewen_daily_report, promotion_daily_report
        3. è¯„ä»·é¡µé¢(æœ€å): 4ä¸ªè¯„ä»·ç›¸å…³ä»»åŠ¡

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            æ‰€æœ‰ä»»åŠ¡çš„æ‰§è¡Œç»“æœåˆ—è¡¨
        """
        print("\n" + "=" * 80)
        print("ğŸš€ é¡µé¢é©±åŠ¨ä»»åŠ¡æ‰§è¡Œæ¨¡å¼")
        print("=" * 80)
        print(f"æ‰§è¡Œé¡ºåº:")
        for i, page_key in enumerate(PAGE_ORDER, 1):
            page_name = self.PAGE_NAME_MAP.get(page_key)
            tasks = PAGE_TASKS.get(page_key, [])
            print(f"   {i}. {page_name}: {', '.join(tasks)}")
        print("=" * 80)

        all_results = []

        try:
            self._disable_proxy()
            self._load_account_info()
            self.start_browser()

            for page_key in PAGE_ORDER:
                page_name = self.PAGE_NAME_MAP.get(page_key)

                # è·³è½¬åˆ°é¡µé¢
                if not self.navigate_to_page(page_key):
                    # è·³è½¬å¤±è´¥ï¼Œè·³è¿‡è¯¥é¡µé¢çš„ä»»åŠ¡
                    print(f"âš ï¸ è·³è¿‡ {page_name} çš„ä»»åŠ¡")
                    for task_name in PAGE_TASKS.get(page_key, []):
                        all_results.append({
                            "task_name": task_name,
                            "success": False,
                            "record_count": 0,
                            "error_message": f"é¡µé¢è·³è½¬å¤±è´¥"
                        })
                    continue

                # æ‰§è¡Œè¯¥é¡µé¢çš„ä»»åŠ¡
                results = self.execute_page_tasks(page_key, start_date, end_date)
                all_results.extend(results)

                # é¡µé¢é—´éšæœºå»¶è¿Ÿ
                random_delay(3, 5)

        except Exception as e:
            error_msg = str(e)
            print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {error_msg}")
            import traceback
            traceback.print_exc()

            # å¦‚æœæ˜¯ç™»å½•å¤±è´¥ï¼ŒåŒæ—¶ä¸ŠæŠ¥æ—¥å¿—åˆ°ä¸¤ä¸ªæ¥å£
            if "ç™»å½•å¤±è´¥" in error_msg or "Cookieç™»å½•å¤±è´¥" in error_msg:
                print(f"\nğŸ“¤ ä¸ŠæŠ¥ç™»å½•å¤±è´¥æ—¥å¿—...")
                # ä¸ŠæŠ¥åˆ° /api/log
                log_failure(self.account_name, 0, "login_check", start_date, end_date, error_msg)
                # ä¸ŠæŠ¥åˆ° /api/account_task/update_batch
                upload_task_status_batch(self.account_name, start_date, end_date, [{
                    'task_name': 'login_check',
                    'success': False,
                    'record_count': 0,
                    'error_message': error_msg
                }])
        finally:
            self.stop_browser()

        return all_results


def run_page_driven_tasks(account_name: str, start_date: str, end_date: str, headless: bool = True) -> List[Dict[str, Any]]:
    """æ‰§è¡Œé¡µé¢é©±åŠ¨çš„ä»»åŠ¡

    è¿™æ˜¯é¡µé¢é©±åŠ¨æ¨¡å¼çš„å…¥å£å‡½æ•°ï¼Œä¼šæŒ‰ç…§ä»¥ä¸‹é¡ºåºæ‰§è¡Œ:
    1. è·³è½¬å®¢æµåˆ†æé¡µé¢ â†’ æ‰§è¡Œ store_stats (å…ˆæ‰§è¡Œï¼Œæ›´æ–°ç­¾å)
    2. è·³è½¬æŠ¥è¡¨é¡µé¢ â†’ æ‰§è¡Œ kewen_daily_report, promotion_daily_report
    3. è·³è½¬è¯„ä»·é¡µé¢ â†’ æ‰§è¡Œ 4ä¸ªè¯„ä»·ä»»åŠ¡ (æœ€åæ‰§è¡Œ)

    Args:
        account_name: è´¦æˆ·åç§°
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼

    Returns:
        æ‰€æœ‰ä»»åŠ¡çš„æ‰§è¡Œç»“æœåˆ—è¡¨
    """
    if not PLAYWRIGHT_AVAILABLE:
        print("âŒ Playwrightæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨é¡µé¢é©±åŠ¨æ¨¡å¼")
        return []

    executor = PageDrivenTaskExecutor(account_name, headless=headless)
    return executor.run_all_tasks(start_date, end_date)


# ============================================================================
# ä»»åŠ¡æ˜ å°„å’Œä¸»å‡½æ•°
# ============================================================================
TASK_MAP = {
    'store_stats': run_store_stats,
    'kewen_daily_report': run_kewen_daily_report,
    'promotion_daily_report': run_promotion_daily_report,
    'review_detail_dianping': run_review_detail_dianping,
    'review_detail_meituan': run_review_detail_meituan,
    'review_summary_dianping': run_review_summary_dianping,
    'review_summary_meituan': run_review_summary_meituan,
}


# ============================================================================
# ä»»åŠ¡è°ƒåº¦APIå‡½æ•°
# ============================================================================
def create_task_schedule() -> bool:
    """ç”Ÿæˆä»»åŠ¡è°ƒåº¦

    è°ƒç”¨ post_task_schedule APIï¼Œè‡ªåŠ¨è®¡ç®—æ—¥æœŸï¼š
    - task_date: å½“æ—¥æ—¥æœŸ
    - data_start_date: å‰å¤©æ—¥æœŸ
    - data_end_date: æ˜¨å¤©æ—¥æœŸ

    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    today = datetime.now()
    task_date = today.strftime("%Y-%m-%d")
    data_start_date = (today - timedelta(days=2)).strftime("%Y-%m-%d")
    data_end_date = (today - timedelta(days=1)).strftime("%Y-%m-%d")

    headers = {'Content-Type': 'application/json'}
    json_param = {
        "task_date": task_date,
        "data_start_date": data_start_date,
        "data_end_date": data_end_date
    }
    proxies = {'http': None, 'https': None}

    print(f"\n{'=' * 80}")
    print("ğŸ“… ç”Ÿæˆä»»åŠ¡è°ƒåº¦")
    print(f"{'=' * 80}")
    print(f"   URL: {TASK_SCHEDULE_API_URL}")
    print(f"   task_date (å½“æ—¥): {task_date}")
    print(f"   data_start_date (å‰å¤©): {data_start_date}")
    print(f"   data_end_date (æ˜¨å¤©): {data_end_date}")

    try:
        response = requests.post(
            TASK_SCHEDULE_API_URL,
            headers=headers,
            data=json.dumps(json_param),
            proxies=proxies,
            timeout=30
        )
        print(f"   HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"   å“åº”å†…å®¹: {response.text[:500] if response.text else '(ç©º)'}")

        if response.status_code == 200:
            print("   âœ… ä»»åŠ¡è°ƒåº¦ç”ŸæˆæˆåŠŸ")
            return True
        else:
            print(f"   âŒ ä»»åŠ¡è°ƒåº¦ç”Ÿæˆå¤±è´¥: HTTP {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ ä»»åŠ¡è°ƒåº¦ç”Ÿæˆå¼‚å¸¸: {e}")
        return False


def fetch_task() -> Optional[Dict[str, Any]]:
    """è·å–ä¸€æ¡å¾…æ‰§è¡Œä»»åŠ¡

    è°ƒç”¨ get_task API è·å–ä»»åŠ¡ä¿¡æ¯

    Returns:
        dict: ä»»åŠ¡ä¿¡æ¯ï¼ŒåŒ…å« id, account_id, task_type, data_start_date, data_end_date ç­‰
        None: å¦‚æœæ²¡æœ‰ä»»åŠ¡æˆ–è·å–å¤±è´¥
    """
    headers = {'Content-Type': 'application/json'}
    proxies = {'http': None, 'https': None}

    print(f"\n{'=' * 80}")
    print("ğŸ“‹ è·å–å¾…æ‰§è¡Œä»»åŠ¡")
    print(f"{'=' * 80}")
    print(f"   URL: {GET_TASK_API_URL}")

    try:
        response = requests.post(
            GET_TASK_API_URL,
            json={},
            proxies=proxies,
            timeout=30
        )
        print(f"   HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"   å“åº”å†…å®¹: {response.text[:500] if response.text else '(ç©º)'}")

        if response.status_code == 200:
            result = response.json()
            # APIè¿”å›æ ¼å¼: {"success":true,"data":{...}}
            task_data = result.get('data') if result.get('success') else None
            if task_data:
                print(f"   âœ… è·å–ä»»åŠ¡æˆåŠŸ")
                print(f"   ä»»åŠ¡ID: {task_data.get('id')}")
                print(f"   è´¦æˆ·: {task_data.get('account_id')}")
                print(f"   ä»»åŠ¡ç±»å‹: {task_data.get('task_type')}")
                print(f"   æ•°æ®æ—¥æœŸ: {task_data.get('data_start_date')} è‡³ {task_data.get('data_end_date')}")
                return task_data
            else:
                print("   âš ï¸ æ²¡æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡")
                return None
        else:
            print(f"   âŒ è·å–ä»»åŠ¡å¤±è´¥: HTTP {response.status_code}")
            return None
    except Exception as e:
        print(f"   âŒ è·å–ä»»åŠ¡å¼‚å¸¸: {e}")
        return None


def report_task_callback(task_id: int, status: int, error_message: str, retry_add: int) -> bool:
    """ä¸ŠæŠ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€

    Args:
        task_id: ä»»åŠ¡ID (ä»fetch_taskè·å–)
        status: çŠ¶æ€ (2=å…¨éƒ¨å®Œæˆ, 3=æœ‰ä»»åŠ¡å¤±è´¥)
        error_message: é”™è¯¯ä¿¡æ¯ (status=3æ—¶éœ€è¦å¡«å†™)
        retry_add: é‡è¯•æ¬¡æ•°å¢åŠ  (status=2æ—¶ä¸º0, status=3æ—¶ä¸º1)

    Returns:
        bool: æ˜¯å¦ä¸ŠæŠ¥æˆåŠŸ
    """
    headers = {'Content-Type': 'application/json'}
    json_param = {
        "id": task_id,
        "status": status,
        "error_message": error_message,
        "retry_add": retry_add
    }
    proxies = {'http': None, 'https': None}

    print(f"\n{'=' * 80}")
    print("ğŸ“¤ ä¸ŠæŠ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€")
    print(f"{'=' * 80}")
    print(f"   URL: {TASK_CALLBACK_API_URL}")
    print(f"   ä»»åŠ¡ID: {task_id}")
    print(f"   çŠ¶æ€: {status} ({'å…¨éƒ¨å®Œæˆ' if status == 2 else 'æœ‰ä»»åŠ¡å¤±è´¥'})")
    if error_message:
        print(f"   é”™è¯¯ä¿¡æ¯: {error_message[:200]}...")
    print(f"   retry_add: {retry_add}")

    try:
        response = requests.post(
            TASK_CALLBACK_API_URL,
            headers=headers,
            data=json.dumps(json_param),
            proxies=proxies,
            timeout=30
        )
        print(f"   HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"   å“åº”å†…å®¹: {response.text[:500] if response.text else '(ç©º)'}")

        if response.status_code == 200:
            print("   âœ… ä»»åŠ¡çŠ¶æ€ä¸ŠæŠ¥æˆåŠŸ")
            return True
        else:
            print(f"   âŒ ä»»åŠ¡çŠ¶æ€ä¸ŠæŠ¥å¤±è´¥: HTTP {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ ä»»åŠ¡çŠ¶æ€ä¸ŠæŠ¥å¼‚å¸¸: {e}")
        return False


def reschedule_failed_tasks() -> bool:
    """é‡æ–°è°ƒåº¦å¤±è´¥çš„ä»»åŠ¡

    è°ƒç”¨ reschedule-failed APIï¼Œè®©å¤±è´¥çš„ä»»åŠ¡é‡æ–°è¿›å…¥è°ƒåº¦é˜Ÿåˆ—

    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    headers = {'Content-Type': 'application/json'}
    proxies = {'http': None, 'https': None}

    print(f"\n{'=' * 80}")
    print("ğŸ”„ é‡æ–°è°ƒåº¦å¤±è´¥ä»»åŠ¡")
    print(f"{'=' * 80}")
    print(f"   URL: {RESCHEDULE_FAILED_API_URL}")

    try:
        response = requests.post(
            RESCHEDULE_FAILED_API_URL,
            json={},
            proxies=proxies,
            timeout=30
        )
        print(f"   HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"   å“åº”å†…å®¹: {response.text[:500] if response.text else '(ç©º)'}")

        if response.status_code == 200:
            print("   âœ… å¤±è´¥ä»»åŠ¡é‡æ–°è°ƒåº¦æˆåŠŸ")
            return True
        else:
            print(f"   âŒ å¤±è´¥ä»»åŠ¡é‡æ–°è°ƒåº¦å¤±è´¥: HTTP {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ å¤±è´¥ä»»åŠ¡é‡æ–°è°ƒåº¦å¼‚å¸¸: {e}")
        return False


def validate_date(date_str: str) -> bool:
    """éªŒè¯æ—¥æœŸæ ¼å¼"""
    try:
        datetime.strptime(date_str, '%Y-%m-%d')
        return True
    except ValueError:
        return False


def print_summary(results: List[Dict[str, Any]]):
    """æ‰“å°æ‰§è¡Œæ‘˜è¦"""
    print("\n" + "=" * 80)
    print("æ‰§è¡Œæ‘˜è¦")
    print("=" * 80)

    success_count = sum(1 for r in results if r.get('success'))
    print(f"æ€»ä»»åŠ¡æ•°: {len(results)}, æˆåŠŸ: {success_count}, å¤±è´¥: {len(results) - success_count}")
    print("-" * 40)

    for result in results:
        status = "âœ…" if result.get('success') else "âŒ"
        print(f"{status} {result.get('task_name')}: è®°å½•æ•°={result.get('record_count', 0)}, é”™è¯¯={result.get('error_message', 'æ— ')}")

    print("=" * 80)


def execute_single_task(task_info: Dict[str, Any]) -> bool:
    """æ‰§è¡Œå•ä¸ªä»»åŠ¡

    Args:
        task_info: ä»APIè·å–çš„ä»»åŠ¡ä¿¡æ¯

    Returns:
        bool: ä»»åŠ¡æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
    """
    global ACCOUNT_NAME, START_DATE, END_DATE, TASK, TARGET_DATE

    task_id = task_info.get("id")

    # å¡«å……é…ç½®å˜é‡
    ACCOUNT_NAME = task_info.get("account_id", "")
    START_DATE = task_info.get("data_start_date", "")
    END_DATE = task_info.get("data_end_date", "")
    TASK = task_info.get("task_type", "all")
    TARGET_DATE = ""

    account_name = ACCOUNT_NAME
    start_date = START_DATE
    end_date = END_DATE
    task = TASK

    print(f"\n{'=' * 80}")
    print("ğŸ“Œ ä»»åŠ¡é…ç½®")
    print(f"{'=' * 80}")
    print(f"   ä»»åŠ¡ID: {task_id}")
    print(f"   è´¦æˆ·åç§°: {account_name}")
    print(f"   æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")
    print(f"   ä»»åŠ¡ç±»å‹: {task}")

    # éªŒè¯å‚æ•°
    if not account_name:
        error_msg = "è´¦æˆ·åç§°ä¸ºç©º"
        print(f"âŒ {error_msg}")
        report_task_callback(task_id, status=3, error_message=error_msg, retry_add=1)
        return False

    if not validate_date(start_date) or not validate_date(end_date):
        error_msg = "æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œåº”ä¸º YYYY-MM-DD"
        print(f"âŒ {error_msg}")
        report_task_callback(task_id, status=3, error_message=error_msg, retry_add=1)
        return False

    start = datetime.strptime(start_date, '%Y-%m-%d')
    end = datetime.strptime(end_date, '%Y-%m-%d')
    if start > end:
        error_msg = "å¼€å§‹æ—¥æœŸä¸èƒ½å¤§äºç»“æŸæ—¥æœŸ"
        print(f"âŒ {error_msg}")
        report_task_callback(task_id, status=3, error_message=error_msg, retry_add=1)
        return False

    valid_tasks = ['all'] + list(TASK_MAP.keys())
    if task not in valid_tasks:
        error_msg = f"æ— æ•ˆçš„ä»»åŠ¡åç§°: {task}ï¼Œå¯é€‰å€¼: {', '.join(valid_tasks)}"
        print(f"âŒ {error_msg}")
        report_task_callback(task_id, status=3, error_message=error_msg, retry_add=1)
        return False

    # ========== è·å–å¹³å°è´¦æˆ·ä¿¡æ¯å¹¶æ£€æŸ¥ templates_id ==========
    print(f"\n{'=' * 80}")
    print("ğŸ” æ£€æŸ¥å¹³å°è´¦æˆ·é…ç½®")
    print(f"{'=' * 80}")

    platform_account = get_platform_account(account_name)

    if not platform_account.get('success'):
        error_msg = f"è·å–å¹³å°è´¦æˆ·ä¿¡æ¯å¤±è´¥: {platform_account.get('error_message', 'æœªçŸ¥é”™è¯¯')}"
        print(f"âŒ {error_msg}")
        # åŒæ—¶ä¸ŠæŠ¥åˆ°ä¸¤ä¸ªæ—¥å¿—æ¥å£
        log_failure(account_name, 0, "platform_account_check", start_date, end_date, error_msg)
        upload_task_status_batch(account_name, start_date, end_date, [{
            'task_name': 'platform_account_check',
            'success': False,
            'record_count': 0,
            'error_message': error_msg
        }])
        report_task_callback(task_id, status=3, error_message=error_msg, retry_add=1)
        return False

    templates_id = platform_account.get('templates_id')
    if templates_id == 0 or templates_id is None:
        error_msg = "æ²¡æœ‰æŠ¥è¡¨IDï¼Œæ— æ³•ç»§ç»­æ‰§è¡Œï¼Œè¯·ç¡®è®¤æ˜¯å¦åœ¨æŠ¥è¡¨ä¸­å¿ƒåˆ›å»ºäº†ï¼šKewen_data"
        print(f"âŒ {error_msg}")
        print(f"   templates_id = {templates_id}")
        # åŒæ—¶ä¸ŠæŠ¥åˆ°ä¸¤ä¸ªæ—¥å¿—æ¥å£
        log_failure(account_name, 0, "templates_id_check", start_date, end_date, error_msg)
        upload_task_status_batch(account_name, start_date, end_date, [{
            'task_name': 'templates_id_check',
            'success': False,
            'record_count': 0,
            'error_message': error_msg
        }])
        report_task_callback(task_id, status=3, error_message=error_msg, retry_add=1)
        return False

    print(f"   âœ… templates_id æ£€æŸ¥é€šè¿‡: {templates_id}")

    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡")
    print("=" * 80)
    if task == 'all':
        print(f"æ‰§è¡Œæ¨¡å¼: é¡µé¢é©±åŠ¨æ¨¡å¼ - å…ˆè·³è½¬é¡µé¢å†æ‰§è¡Œä»»åŠ¡")
        print(f"æ‰§è¡Œé¡ºåº:")
        print(f"   1. å®¢æµåˆ†æé¡µé¢: store_stats (å…ˆæ‰§è¡Œï¼Œæ›´æ–°ç­¾å)")
        print(f"   2. æŠ¥è¡¨é¡µé¢: kewen_daily_report, promotion_daily_report")
        print(f"   3. è¯„ä»·é¡µé¢: review_detail_dianping, review_detail_meituan,")
        print(f"                review_summary_dianping, review_summary_meituan")
    print("=" * 80)

    # æ‰§è¡Œä»»åŠ¡
    results = []
    try:
        if task == 'all':
            results = run_page_driven_tasks(
                account_name=account_name,
                start_date=start_date,
                end_date=end_date,
                headless=HEADLESS
            )
        else:
            result = TASK_MAP[task](account_name, start_date, end_date)
            results.append(result)
    except Exception as e:
        error_msg = f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        report_task_callback(task_id, status=3, error_message=error_msg, retry_add=1)
        return False

    print_summary(results)

    # ä¸ŠæŠ¥ä»»åŠ¡çŠ¶æ€
    if task == 'all':
        upload_task_status_batch(account_name, start_date, end_date, results)
    else:
        if results:
            upload_task_status_single(account_name, start_date, end_date, results[0])

    # æ”¶é›†é”™è¯¯å¹¶ä¸ŠæŠ¥ä»»åŠ¡å›è°ƒ
    task_errors = []
    for result in results:
        if not result.get('success'):
            task_name = result.get('task_name', 'æœªçŸ¥ä»»åŠ¡')
            error_msg = result.get('error_message', 'æœªçŸ¥é”™è¯¯')
            task_errors.append(f"[{task_name}] {error_msg}")

    if len(task_errors) == 0:
        report_task_callback(task_id, status=2, error_message="", retry_add=0)
        return True
    else:
        all_errors = "\n".join(task_errors)
        report_task_callback(task_id, status=3, error_message=all_errors, retry_add=1)
        return False


def main():
    """ä¸»å‡½æ•° - å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼

    æŒç»­å¾ªç¯è¿è¡Œï¼Œè‡ªåŠ¨è·å–å¹¶æ‰§è¡Œä»»åŠ¡:
    1. æ£€æŸ¥æ—¶é—´çª—å£ (DEV_MODE=Trueæ—¶24å°æ—¶è¿è¡Œ)
    2. ç”Ÿæˆä»»åŠ¡è°ƒåº¦
    3. è·å–ä»»åŠ¡å¹¶æ‰§è¡Œ
    4. æ— ä»»åŠ¡æ—¶ç­‰å¾…5åˆ†é’Ÿåé‡è¯•
    5. æ”¯æŒ Ctrl+C ä¼˜é›…é€€å‡º
    """
    global _daemon_running

    # ========== åˆå§‹åŒ– ==========
    print("\n" + "=" * 80)
    print("ç¾å›¢ç‚¹è¯„æ•°æ®é‡‡é›†ç³»ç»Ÿ (å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼)")
    print("=" * 80)
    print(f"   è¿è¡Œæ¨¡å¼: {'å¼€å‘æ¨¡å¼ (24å°æ—¶è¿è¡Œ)' if DEV_MODE else f'ç”Ÿäº§æ¨¡å¼ ({WORK_START_HOUR}:00-{WORK_END_HOUR}:00)'}")
    print(f"   æ— ä»»åŠ¡ç­‰å¾…: {NO_TASK_WAIT_SECONDS // 60} åˆ†é’Ÿ")
    print(f"   æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"   çŠ¶æ€ç›®å½•: {STATE_DIR}")
    print(f"   ä¸‹è½½ç›®å½•: {DOWNLOAD_DIR}")
    print("=" * 80)

    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    _setup_signal_handlers()

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    ensure_directories()

    # ç»Ÿè®¡ä¿¡æ¯
    total_tasks = 0
    success_tasks = 0
    failed_tasks = 0

    print("\nğŸš€ å¼€å§‹å®ˆæŠ¤è¿›ç¨‹å¾ªç¯...")
    print("   æŒ‰ Ctrl+C å¯ä¼˜é›…é€€å‡º\n")

    # ========== ä¸»å¾ªç¯ ==========
    while _daemon_running:
        try:
            # ========== Step 1: æ—¶é—´çª—å£æ£€æŸ¥ ==========
            if not is_in_work_window():
                wait_seconds = seconds_until_work_start()
                hours = wait_seconds // 3600
                minutes = (wait_seconds % 3600) // 60
                print(f"\n{'=' * 60}")
                print(f"ğŸ’¤ å½“å‰éå·¥ä½œæ—¶é—´ ({WORK_START_HOUR}:00-{WORK_END_HOUR}:00)")
                print(f"   å°†åœ¨ {hours}å°æ—¶{minutes}åˆ†é’Ÿ åå¼€å§‹å·¥ä½œ...")
                print(f"{'=' * 60}")

                if not interruptible_sleep(wait_seconds):
                    break  # æ”¶åˆ°é€€å‡ºä¿¡å·
                continue

            # ========== Step 2: ç”Ÿæˆä»»åŠ¡è°ƒåº¦ ==========
            create_task_schedule()
            time.sleep(5)

            # ========== Step 3: è·å–ä»»åŠ¡ ==========
            task_info = fetch_task()

            if not task_info:
                print(f"\nâ³ æš‚æ— å¾…æ‰§è¡Œä»»åŠ¡ï¼Œ{NO_TASK_WAIT_SECONDS // 60}åˆ†é’Ÿåé‡è¯•...")
                reschedule_failed_tasks()

                if not interruptible_sleep(NO_TASK_WAIT_SECONDS):
                    break  # æ”¶åˆ°é€€å‡ºä¿¡å·
                continue

            # ========== Step 4: æ‰§è¡Œä»»åŠ¡ ==========
            total_tasks += 1
            success = execute_single_task(task_info)

            if success:
                success_tasks += 1
            else:
                failed_tasks += 1

            # ========== Step 5: é‡æ–°è°ƒåº¦å¤±è´¥ä»»åŠ¡ ==========
            reschedule_failed_tasks()

            # æ‰“å°å½“å‰ç»Ÿè®¡
            print(f"\nğŸ“Š ç´¯è®¡ç»Ÿè®¡: æ€»ä»»åŠ¡={total_tasks}, æˆåŠŸ={success_tasks}, å¤±è´¥={failed_tasks}")

            # çŸ­æš‚ç­‰å¾…åç»§ç»­ä¸‹ä¸€è½®
            time.sleep(2)

        except KeyboardInterrupt:
            # äºŒæ¬¡ Ctrl+C å¼ºåˆ¶é€€å‡º
            print("\nâš ï¸ å†æ¬¡æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œå¼ºåˆ¶é€€å‡º...")
            break
        except Exception as e:
            print(f"\nâŒ ä¸»å¾ªç¯å‘ç”Ÿå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            # ç­‰å¾…ä¸€æ®µæ—¶é—´åç»§ç»­
            print(f"   å°†åœ¨60ç§’åç»§ç»­è¿è¡Œ...")
            if not interruptible_sleep(60):
                break

    # ========== é€€å‡º ==========
    print("\n" + "=" * 80)
    print("âœ… å®ˆæŠ¤è¿›ç¨‹æ­£å¸¸é€€å‡º")
    print(f"   æ€»ä»»åŠ¡: {total_tasks}, æˆåŠŸ: {success_tasks}, å¤±è´¥: {failed_tasks}")
    print("=" * 80)


if __name__ == "__main__":
    main()

